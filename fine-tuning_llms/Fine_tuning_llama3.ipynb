{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:59.390190Z",
     "start_time": "2024-07-29T02:19:55.386314Z"
    }
   },
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "from accelerate import Accelerator"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wandb will be done later",
   "id": "5821b9c54bc8063"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:29.074897Z",
     "start_time": "2024-07-29T02:19:59.390190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "access_token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# QLoRA config\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n"
   ],
   "id": "971bc197ea24f8c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f08824f267d6442eb2b01d1d613d7b1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:29.130267Z",
     "start_time": "2024-07-29T02:20:29.074897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "ece399d925f96143",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:30.074570Z",
     "start_time": "2024-07-29T02:20:29.131270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare_model(model)"
   ],
   "id": "5247f447282f0c8b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Datasets preparation",
   "id": "1d752869d82ccf4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:30.078407Z",
     "start_time": "2024-07-29T02:20:30.075576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_url = \"./small_dataset/train.jsonl\"\n",
    "test_dataset_url =\"./small_dataset/test.jsonl\"\n",
    "validation_dataset_url =\"./small_dataset/validation.jsonl\""
   ],
   "id": "77bd963076218c38",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Datasets loading",
   "id": "855a5b65ce277a41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:30.783601Z",
     "start_time": "2024-07-29T02:20:30.080411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_files = {\n",
    "    'train': train_dataset_url,\n",
    "    'test': test_dataset_url,\n",
    "    'validation': validation_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ],
   "id": "1c1dc761acc84d8f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Datasets tokenization",
   "id": "524c4f8dd3385b3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:31.904730Z",
     "start_time": "2024-07-29T02:20:30.783601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['output']\n",
    "    max_length = 2048\n",
    "    model_input = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_input['labels'] = labels['input_ids']\n",
    "    return model_input\n",
    "\n",
    "trained_data = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_data = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_data = test_dataset.map(tokenize_function, batched=True)"
   ],
   "id": "b3c3fab4af283118",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a075d7d064d44ce9b04a566ec5de561"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4144: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training arguments",
   "id": "7d490b9223bd81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:31.943986Z",
     "start_time": "2024-07-29T02:20:31.905734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1\n",
    "epochs = 5\n",
    "output_dir = 't5_datasets_class1/results'\n",
    "logs_dir = 't5_datasets_class1/logs'\n",
    "\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    logging_dir=logs_dir,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    max_seq_length= 2048,\n",
    "    do_eval=True, \n",
    "    bf16=True \n",
    ")"
   ],
   "id": "7c6d1a0af6cbe0b1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluation metrics",
   "id": "f2927cfa765a89aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:34.215364Z",
     "start_time": "2024-07-29T02:20:31.944993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "def compute_metrics(eval_pred):\n",
    "    metrics, labels = eval_pred\n",
    "    predictions = np.argmax(metrics, axis=-1)\n",
    "\n",
    "    return perplexity.compute(predictions=predictions, model_id='Meta-Llama-3.1-8B-Instruct')\n"
   ],
   "id": "de8025404d43a7b3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training",
   "id": "93e8096e5d0548cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:20:34.234643Z",
     "start_time": "2024-07-29T02:20:34.215459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=trained_data,\n",
    "    eval_dataset=validation_data,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "7b23a5420a85c808",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:29:42.809478Z",
     "start_time": "2024-07-29T02:20:34.235647Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "68f53c664c78e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/1790 07:09 < 4:13:48, 0.11 it/s, Epoch 0.14/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/1790 07:09 < 4:13:48, 0.11 it/s, Epoch 0.14/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/77 01:27 < 14:20, 0.08 it/s]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.81 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:451\u001B[0m, in \u001B[0;36mSFTTrainer.train\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneftune_noise_alpha \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trainer_supports_neftune:\n\u001B[0;32m    449\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trl_activate_neftune(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel)\n\u001B[1;32m--> 451\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001B[39;00m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneftune_noise_alpha \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trainer_supports_neftune:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer.py:1938\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1936\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1937\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1938\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[0;32m   1939\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[0;32m   1940\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[0;32m   1941\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[0;32m   1942\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[0;32m   1943\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer.py:2356\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m steps_skipped) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[0;32m   2354\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m-> 2356\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001B[0;32m   2357\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2358\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer.py:2804\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2802\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_evaluate:\n\u001B[1;32m-> 2804\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate(trial, ignore_keys_for_eval)\n\u001B[0;32m   2806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[0;32m   2807\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_checkpoint(model, trial, metrics\u001B[38;5;241m=\u001B[39mmetrics)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer.py:2761\u001B[0m, in \u001B[0;36mTrainer._evaluate\u001B[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001B[0m\n\u001B[0;32m   2760\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_evaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, ignore_keys_for_eval, skip_scheduler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m-> 2761\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(ignore_keys\u001B[38;5;241m=\u001B[39mignore_keys_for_eval)\n\u001B[0;32m   2762\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_report_to_hp_search(trial, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step, metrics)\n\u001B[0;32m   2764\u001B[0m     \u001B[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer.py:3666\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   3663\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   3665\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[1;32m-> 3666\u001B[0m output \u001B[38;5;241m=\u001B[39m eval_loop(\n\u001B[0;32m   3667\u001B[0m     eval_dataloader,\n\u001B[0;32m   3668\u001B[0m     description\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3669\u001B[0m     \u001B[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;00m\n\u001B[0;32m   3670\u001B[0m     \u001B[38;5;66;03m# self.args.prediction_loss_only\u001B[39;00m\n\u001B[0;32m   3671\u001B[0m     prediction_loss_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   3672\u001B[0m     ignore_keys\u001B[38;5;241m=\u001B[39mignore_keys,\n\u001B[0;32m   3673\u001B[0m     metric_key_prefix\u001B[38;5;241m=\u001B[39mmetric_key_prefix,\n\u001B[0;32m   3674\u001B[0m )\n\u001B[0;32m   3676\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[0;32m   3677\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer.py:3882\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   3880\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function((logits))\n\u001B[0;32m   3881\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mbatch_eval_metrics \u001B[38;5;129;01mor\u001B[39;00m description \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrediction\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 3882\u001B[0m         all_preds\u001B[38;5;241m.\u001B[39madd(logits)\n\u001B[0;32m   3883\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3884\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function((labels))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:328\u001B[0m, in \u001B[0;36mEvalLoopContainer.add\u001B[1;34m(self, tensors)\u001B[0m\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_nested_concat \u001B[38;5;28;01melse\u001B[39;00m [tensors]\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_nested_concat:\n\u001B[1;32m--> 328\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m nested_concat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors, tensors, padding_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_index)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors\u001B[38;5;241m.\u001B[39mappend(tensors)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:142\u001B[0m, in \u001B[0;36mnested_concat\u001B[1;34m(tensors, new_tensors, padding_index)\u001B[0m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(nested_concat(t, n, padding_index\u001B[38;5;241m=\u001B[39mpadding_index) \u001B[38;5;28;01mfor\u001B[39;00m t, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(tensors, new_tensors))\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001B[38;5;241m=\u001B[39mpadding_index)\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, Mapping):\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(\n\u001B[0;32m    145\u001B[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001B[38;5;241m=\u001B[39mpadding_index) \u001B[38;5;28;01mfor\u001B[39;00m k, t \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m    146\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:100\u001B[0m, in \u001B[0;36mtorch_pad_and_concatenate\u001B[1;34m(tensor1, tensor2, padding_index)\u001B[0m\n\u001B[0;32m     97\u001B[0m tensor2 \u001B[38;5;241m=\u001B[39m atleast_1d(tensor2)\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(tensor1\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat((tensor1, tensor2), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    102\u001B[0m \u001B[38;5;66;03m# Let's figure out the new shape\u001B[39;00m\n\u001B[0;32m    103\u001B[0m new_shape \u001B[38;5;241m=\u001B[39m (tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mmax\u001B[39m(tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:]\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 8.81 GiB. GPU "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "saving the model",
   "id": "57c37a2e633b163d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:29:42.810483Z",
     "start_time": "2024-07-29T02:29:42.810483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = 't5_model_and_result/model'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "id": "8b6b57b5a75e834d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
