{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T10:02:10.183761Z",
     "start_time": "2024-08-11T10:02:09.050591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os"
   ],
   "id": "2df2de4a6951e750",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:13:15.459616Z",
     "start_time": "2024-08-10T23:13:15.448046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#to login for token\n",
    "from huggingface_hub import login\n",
    "login()"
   ],
   "id": "3320a89aa4b039aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67863cad05fe4bf296246dded9a46b87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the tokenizer to tokenize the text data",
   "id": "5d8a7eb9c1d940f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T10:02:10.724791Z",
     "start_time": "2024-08-11T10:02:10.184543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"google/gemma-2-2b\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ],
   "id": "c246a56962c6b21b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KNOW THE DATA : \n",
    "### 1. Do a statistical analysis of the data. \n",
    "### 2. Figure out how much tokens are used for non-important data"
   ],
   "id": "63e001dce630e5c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Statistical Analysis\n",
    "Counts the number of tokens in a string"
   ],
   "id": "16094be54459ee97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T10:02:16.933366Z",
     "start_time": "2024-08-11T10:02:16.930504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using the provided tokenizer.\n",
    "\n",
    "    :param text: The input text to tokenize.\n",
    "    :return: The number of tokens in the text.\n",
    "    \"\"\"\n",
    "    encoded_tokens = tokenizer.encode(text)\n",
    "    return len(encoded_tokens)"
   ],
   "id": "2fa5adda5d49d5d7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "counts the number of tokens in an xmi file",
   "id": "885a3d98e1135ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T10:02:19.237649Z",
     "start_time": "2024-08-11T10:02:19.235866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def count_tokens_in_file(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Reads the content of the file and returns the number of tokens using the provided tokenizer.\n",
    "\n",
    "    :param file_path: The path to the file.\n",
    "    :return: The number of tokens in the file content.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return count_tokens(content)"
   ],
   "id": "e5f32a55b0a62949",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Counts the number of tokens in a folder and make a statistics with categories",
   "id": "b5e90459e37b3232"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:47:17.585762Z",
     "start_time": "2024-08-10T23:47:17.583215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens_in_folder(folder_path: str):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens for each file in a folder and categorizes them.\n",
    "\n",
    "    :param folder_path: The path to the folder containing the files.\n",
    "    :param tokenizer: The tokenizer to use for encoding the text.\n",
    "    \"\"\"\n",
    "    token_counts = {\n",
    "        '<1000': 0,\n",
    "        '<2000': 0,\n",
    "        '<3000': 0,\n",
    "        '<4000': 0,\n",
    "        '<5000': 0,\n",
    "        '<6000': 0,\n",
    "        '<7000': 0,\n",
    "        '<=8192': 0,\n",
    "        '>8192': 0\n",
    "    }\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            num_tokens = count_tokens_in_file(file_path)\n",
    "            if num_tokens < 1000:\n",
    "                token_counts['<1000'] += 1\n",
    "            elif num_tokens < 2000:\n",
    "                token_counts['<2000'] += 1\n",
    "            elif num_tokens < 3000:\n",
    "                token_counts['<3000'] += 1\n",
    "            elif num_tokens < 4000:\n",
    "                token_counts['<4000'] += 1\n",
    "            elif num_tokens < 5000:\n",
    "                token_counts['<5000'] += 1\n",
    "            elif num_tokens < 6000:\n",
    "                token_counts['<6000'] += 1\n",
    "            elif num_tokens < 7000:\n",
    "                token_counts['<7000'] += 1\n",
    "            elif num_tokens <= 8192:\n",
    "                token_counts['<=8192'] += 1\n",
    "            elif num_tokens > 8192:\n",
    "                token_counts['>8192'] += 1\n",
    "                \n",
    "    for category, count in token_counts.items():\n",
    "            print(f\"Number of xmi files with tokens {category}: {count}\")            "
   ],
   "id": "46de381f36ce48a5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:47:18.137407Z",
     "start_time": "2024-08-10T23:47:18.134981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_files_in_folder(folder_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of files in a folder.\n",
    "\n",
    "    :param folder_path: The path to the folder.\n",
    "    :return: The number of files in the folder.\n",
    "    \"\"\"\n",
    "    file_count = 0\n",
    "    for item in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, item)):\n",
    "            file_count += 1\n",
    "    return file_count"
   ],
   "id": "83d57a63217fdeca",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Execution !!!",
   "id": "557c33f2ff7ccff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:48:25.994436Z",
     "start_time": "2024-08-10T23:47:19.436600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path = 'modelset/raw-data/repo-genmymodel-uml/data'\n",
    "numbers = count_files_in_folder(folder_path)\n",
    "print(f\"There are {numbers} files in the folder.\")\n",
    "count_tokens_in_folder(folder_path)"
   ],
   "id": "c00cdd7d76b8cc77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5120 files in the folder.\n",
      "Number of xmi files with tokens <1000: 0\n",
      "Number of xmi files with tokens <2000: 0\n",
      "Number of xmi files with tokens <3000: 0\n",
      "Number of xmi files with tokens <4000: 0\n",
      "Number of xmi files with tokens <5000: 78\n",
      "Number of xmi files with tokens <6000: 97\n",
      "Number of xmi files with tokens <7000: 171\n",
      "Number of xmi files with tokens <=8192: 182\n",
      "Number of xmi files with tokens >8192: 4592\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How many tokens are used for non-important data?\n",
    "This is divided into minor points:\n",
    "* How many tokens are used before encountering the first class?\n",
    "* How many tokens are used to describe one class (with all its attributes)?\n",
    "* How many tokens are required for generalization?"
   ],
   "id": "5e14a82cb6d97c25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How many tokens are used before encountering the first class?",
   "id": "c9679452f8a5bd3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T15:37:38.708467Z",
     "start_time": "2024-08-09T15:37:38.702939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_until_class(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of the file until it encounters the string 'packagedElement xsi:type=\"uml:Class\"'.\n",
    "\n",
    "    :param file_path: The path to the file.\n",
    "    :return: The content read until the specified string is found.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    search_string = 'packagedElement xsi:type=\"uml:Class\"'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            content.append(line)\n",
    "            if search_string in line:\n",
    "                break\n",
    "    \n",
    "    return ''.join(content)"
   ],
   "id": "592e879f08b8491c",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count the number of tokens before the first class",
   "id": "a9671a06929b70ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T15:43:13.024037Z",
     "start_time": "2024-08-09T15:43:13.021383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens_before_first_class(folder_path: str):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens for each file in a folder before the first class\n",
    "\n",
    "    :param folder_path: The path to the folder containing the files.\n",
    "    :param tokenizer: The tokenizer to use for encoding the text.\n",
    "    \"\"\"\n",
    "    token_counts = {\n",
    "        '<100': 0,\n",
    "        '<200': 0,\n",
    "        '<300': 0,\n",
    "        '<400': 0,\n",
    "        '<500': 0,\n",
    "        '<600': 0,\n",
    "        '<700': 0,\n",
    "        '<800': 0,\n",
    "        '<900' : 0,\n",
    "        '>=900' : 0,\n",
    "    }\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            xmi_before_class = read_until_class(file_path)\n",
    "            num_tokens = count_tokens(xmi_before_class)\n",
    "            if num_tokens < 100:\n",
    "                token_counts['<100'] += 1\n",
    "            elif num_tokens < 200:\n",
    "                token_counts['<200'] += 1\n",
    "            elif num_tokens < 300:\n",
    "                token_counts['<300'] += 1\n",
    "            elif num_tokens < 400:\n",
    "                token_counts['<400'] += 1\n",
    "            elif num_tokens < 500:\n",
    "                token_counts['<500'] += 1\n",
    "            elif num_tokens < 600:\n",
    "                token_counts['<600'] += 1\n",
    "            elif num_tokens < 700:\n",
    "                token_counts['<700'] += 1\n",
    "            elif num_tokens < 800:\n",
    "                token_counts['<800'] += 1\n",
    "            elif num_tokens < 900:\n",
    "                token_counts['<900'] += 1\n",
    "            elif num_tokens >= 900:\n",
    "                token_counts['>=900'] += 1\n",
    "\n",
    "    for category, count in token_counts.items():\n",
    "        print(f\"Number of xmi files with tokens before first class {category}: {count}\")  "
   ],
   "id": "694421bc14b52945",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T15:43:45.368401Z",
     "start_time": "2024-08-09T15:43:13.539233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path = 'modelset/raw-data/repo-genmymodel-uml/data'\n",
    "numbers = count_files_in_folder(folder_path)\n",
    "print(f\"There are {numbers} files in the folder.\")\n",
    "count_tokens_before_first_class(folder_path)"
   ],
   "id": "a993b0854c3a230b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5120 files in the folder.\n",
      "Number of xmi files with tokens before first class <100: 0\n",
      "Number of xmi files with tokens before first class <200: 0\n",
      "Number of xmi files with tokens before first class <300: 0\n",
      "Number of xmi files with tokens before first class <400: 0\n",
      "Number of xmi files with tokens before first class <500: 18\n",
      "Number of xmi files with tokens before first class <600: 599\n",
      "Number of xmi files with tokens before first class <700: 366\n",
      "Number of xmi files with tokens before first class <800: 323\n",
      "Number of xmi files with tokens before first class <900: 77\n",
      "Number of xmi files with tokens before first class >=900: 3737\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How many tokens does a Class with attributes take?",
   "id": "cd7ffd82290c6f7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:42:22.575610Z",
     "start_time": "2024-08-10T23:42:22.571902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from extraction_remove_one_class_removed import parse_xmi, namespaces_org, get_classes, get_class_attributes, ET\n",
    "\n",
    "def count_tokens_of_classes_with_att(folder_path: str):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens for each file in a folder before the first class\n",
    "\n",
    "    :param folder_path: The path to the folder containing the files.\n",
    "    :param tokenizer: The tokenizer to use for encoding the text.\n",
    "    \"\"\"\n",
    "    token_sums = {\n",
    "        'class_with_att_num_1': 0,\n",
    "        'class_with_att_num_2': 0,\n",
    "        'class_with_att_num_3': 0,\n",
    "        'class_with_att_num_4': 0,\n",
    "        'class_with_att_num_5': 0,\n",
    "        'class_with_att_num_6': 0,\n",
    "        'class_with_att_num_>=7': 0\n",
    "    }\n",
    "\n",
    "    class_counts = {\n",
    "        'class_with_att_num_1': 0,\n",
    "        'class_with_att_num_2': 0,\n",
    "        'class_with_att_num_3': 0,\n",
    "        'class_with_att_num_4': 0,\n",
    "        'class_with_att_num_5': 0,\n",
    "        'class_with_att_num_6': 0,\n",
    "        'class_with_att_num_>=7': 0\n",
    "    }\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        root = parse_xmi(file_path)\n",
    "        classes = get_classes(root,namespaces_org)\n",
    "        for class_elem in classes:\n",
    "           attributes = get_class_attributes(class_elem)\n",
    "           num_attributes = len(attributes)\n",
    "           class_text = ET.tostring(class_elem, encoding='unicode')\n",
    "           attributes_text = ''.join([ET.tostring(attr, encoding='unicode') for attr in attributes])\n",
    "           combined_text = class_text + attributes_text\n",
    "           num_tokens = count_tokens(combined_text)\n",
    "\n",
    "        if num_attributes == 1:\n",
    "            token_sums['class_with_att_num_1'] += num_tokens\n",
    "            class_counts['class_with_att_num_1'] += 1\n",
    "        elif num_attributes == 2:\n",
    "            token_sums['class_with_att_num_2'] += num_tokens\n",
    "            class_counts['class_with_att_num_2'] += 1\n",
    "        elif num_attributes == 3:\n",
    "            token_sums['class_with_att_num_3'] += num_tokens\n",
    "            class_counts['class_with_att_num_3'] += 1\n",
    "        elif num_attributes == 4:\n",
    "            token_sums['class_with_att_num_4'] += num_tokens\n",
    "            class_counts['class_with_att_num_4'] += 1\n",
    "        elif num_attributes == 5:\n",
    "            token_sums['class_with_att_num_5'] += num_tokens\n",
    "            class_counts['class_with_att_num_5'] += 1\n",
    "        elif num_attributes == 6:\n",
    "            token_sums['class_with_att_num_6'] += num_tokens\n",
    "            class_counts['class_with_att_num_6'] += 1\n",
    "        else:\n",
    "            token_sums['class_with_att_num_>=7'] += num_tokens\n",
    "            class_counts['class_with_att_num_>=7'] += 1\n",
    "\n",
    "    for category in token_sums:\n",
    "        if class_counts[category] > 0:\n",
    "            average_tokens = token_sums[category] / class_counts[category]\n",
    "        else:\n",
    "            average_tokens = 0\n",
    "        print(f\"Average number of tokens for {category}: {average_tokens}\")      \n",
    "        "
   ],
   "id": "5123f9b2cdffb22",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:42:52.593910Z",
     "start_time": "2024-08-10T23:42:22.733490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path = 'modelset/raw-data/repo-genmymodel-uml/data'\n",
    "count_tokens_of_classes_with_att(folder_path)"
   ],
   "id": "70e79b0f7fad54af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens for class_with_att_num_1: 1229.5967741935483\n",
      "Average number of tokens for class_with_att_num_2: 1678.4948741845294\n",
      "Average number of tokens for class_with_att_num_3: 2093.126168224299\n",
      "Average number of tokens for class_with_att_num_4: 3182.4534161490683\n",
      "Average number of tokens for class_with_att_num_5: 3064.060606060606\n",
      "Average number of tokens for class_with_att_num_6: 3772.4310344827586\n",
      "Average number of tokens for class_with_att_num_>=7: 822.687265278252\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simple example with a JSON file",
   "id": "623783c6f38b0b2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T10:04:39.713040Z",
     "start_time": "2024-08-11T10:04:39.700517Z"
    }
   },
   "cell_type": "code",
   "source": "count_tokens_in_file('modelset/graph/repo-genmymodel-uml/data/0a3c61d9-aec4-4842-9334-ea5ac4edc7ef.xmi/0a3c61d9-aec4-4842-9334-ea5ac4edc7ef.json')",
   "id": "33fc633242fdfafa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
