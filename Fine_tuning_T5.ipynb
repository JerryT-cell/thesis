{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-08T18:33:38.540296Z",
     "start_time": "2024-08-08T18:33:37.035924Z"
    }
   },
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer,  DataCollatorForSeq2Seq)\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get Data",
   "id": "b7f5349c23da5dbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T18:33:38.984295Z",
     "start_time": "2024-08-08T18:33:38.541098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_files = {\n",
    "    'train': 'datasets_train_valid_test/test.jsonl',\n",
    "    'test': 'datasets_train_valid_test/test.jsonl',\n",
    "    'validation': 'datasets_train_valid_test/test.jsonl'\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ],
   "id": "383fcb3161e7942c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T18:33:38.990272Z",
     "start_time": "2024-08-08T18:33:38.985526Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "4e08e0c0cadcde22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the token and the T5 model",
   "id": "a975af5c54a27cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T18:33:39.449733Z",
     "start_time": "2024-08-08T18:33:38.991919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'google-t5/t5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ],
   "id": "85b0e929047b8c61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "model = T5ForConditionalGeneration.from_pretrained(model_name)",
   "id": "cfcdb56502dd91fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "preprocess the data",
   "id": "e35c1cd86d2f4627"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T18:34:04.260181Z",
     "start_time": "2024-08-08T18:33:47.659170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(data_p):\n",
    "    prefix = \"complete: \"\n",
    "    max_length = 512\n",
    "    inputs = [prefix + d for d in data_p['input']]\n",
    "    targets = [d for d in data_p['output']]\n",
    "    model_input = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        \n",
    "    model_input['labels'] = labels['input_ids']  \n",
    "    return model_input\n",
    "\n"
   ],
   "id": "4956a905208daf26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T18:34:58.286993Z",
     "start_time": "2024-08-08T18:34:54.674744Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "727e622f3b07fb3b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:37.203596Z",
     "start_time": "2024-07-13T03:01:36.965926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_data = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_data = validation_dataset.map(preprocess_function, batched=True)\n",
    "test_data = test_dataset.map(preprocess_function, batched=True)"
   ],
   "id": "924e7d00b0a2330d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:42.125841Z",
     "start_time": "2024-07-13T03:01:42.115825Z"
    }
   },
   "cell_type": "code",
   "source": "type(validation_data)",
   "id": "668c5748d093d927",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "training arguments",
   "id": "fb49d653b28a3e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:44.725643Z",
     "start_time": "2024-07-13T03:01:44.670404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "epochs = 5\n",
    "max_length = 512\n",
    "output_dir = 't5_data/results'\n",
    "logs_dir = 't5_data/logs'\n",
    "\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    evaluation_strategy='steps',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_dir=logs_dir,\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    save_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    #predict_with_generate=True,\n",
    "    warmup_steps=500\n",
    ")"
   ],
   "id": "ad5e5606fa8bc4e2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The trainer",
   "id": "28b7701b83a442e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:48.719712Z",
     "start_time": "2024-07-13T03:01:48.491400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset= trained_data,\n",
    "    eval_dataset= validation_data,\n",
    ")"
   ],
   "id": "254f44b871964903",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPU",
   "id": "933c3baa7d75bc5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3eebc8e608fcd685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:51.802948Z",
     "start_time": "2024-07-13T03:01:51.787877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "id": "914474bc8c4d76db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the model",
   "id": "38d991cdf7145a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T09:00:27.689598Z",
     "start_time": "2024-07-13T03:01:53.060007Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "47473d537caa3dd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1121' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1120/1120 5:58:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.301600</td>\n",
       "      <td>0.289887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.065977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.049841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.040206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.034990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1120' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1120/1120 5:58:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.301600</td>\n",
       "      <td>0.289887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.065977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.049841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.040206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.034990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1120, training_loss=0.831597021647862, metrics={'train_runtime': 21514.5043, 'train_samples_per_second': 0.833, 'train_steps_per_second': 0.052, 'total_flos': 1.09125253988352e+16, 'train_loss': 0.831597021647862, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "save model  and tokenizer",
   "id": "593fd50ca7af1ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T11:00:44.651543Z",
     "start_time": "2024-07-13T11:00:43.661306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = 't5_data/model'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "id": "3ebbaf7b4b5d86c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_data/model\\\\tokenizer_config.json',\n",
       " 't5_data/model\\\\special_tokens_map.json',\n",
       " 't5_data/model\\\\spiece.model',\n",
       " 't5_data/model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inferencing",
   "id": "3d09c73ca070ac4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
