{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T11:18:32.775583Z",
     "start_time": "2024-07-16T11:18:24.560984Z"
    }
   },
   "source": [
    "import nltk\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer,  DataCollatorForSeq2Seq)\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get Data",
   "id": "b7f5349c23da5dbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:20:18.077726Z",
     "start_time": "2024-07-16T11:20:17.235297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_files = {\n",
    "    'train': 't5_datasets/train.jsonl',\n",
    "    'test': 't5_datasets/test.jsonl',\n",
    "    'validation': 't5_datasets/validation.jsonl'\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ],
   "id": "383fcb3161e7942c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:26.862959Z",
     "start_time": "2024-07-13T03:01:26.857148Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "4e08e0c0cadcde22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 3584\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the token and the T5 model",
   "id": "a975af5c54a27cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:32.641837Z",
     "start_time": "2024-07-13T03:01:30.465559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'google-t5/t5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ],
   "id": "cfcdb56502dd91fc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "preprocess the data",
   "id": "e35c1cd86d2f4627"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:35.567008Z",
     "start_time": "2024-07-13T03:01:35.563802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(data_p):\n",
    "    prefix = \"complete: \"\n",
    "    max_length = 512\n",
    "    inputs = [prefix + d for d in data_p['input']]\n",
    "    targets = [d for d in data_p['output']]\n",
    "    model_input = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        \n",
    "    model_input['labels'] = labels['input_ids']  \n",
    "    return model_input"
   ],
   "id": "4956a905208daf26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:37.203596Z",
     "start_time": "2024-07-13T03:01:36.965926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_data = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_data = validation_dataset.map(preprocess_function, batched=True)\n",
    "test_data = test_dataset.map(preprocess_function, batched=True)"
   ],
   "id": "924e7d00b0a2330d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:42.125841Z",
     "start_time": "2024-07-13T03:01:42.115825Z"
    }
   },
   "cell_type": "code",
   "source": "type(validation_data)",
   "id": "668c5748d093d927",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "training arguments",
   "id": "fb49d653b28a3e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:44.725643Z",
     "start_time": "2024-07-13T03:01:44.670404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "epochs = 5\n",
    "max_length = 512\n",
    "output_dir = 't5_data/results'\n",
    "logs_dir = 't5_data/logs'\n",
    "\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    evaluation_strategy='steps',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_dir=logs_dir,\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    save_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    #predict_with_generate=True,\n",
    "    warmup_steps=500\n",
    ")"
   ],
   "id": "ad5e5606fa8bc4e2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The trainer",
   "id": "28b7701b83a442e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:48.719712Z",
     "start_time": "2024-07-13T03:01:48.491400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset= trained_data,\n",
    "    eval_dataset= validation_data,\n",
    ")"
   ],
   "id": "254f44b871964903",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPU",
   "id": "933c3baa7d75bc5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3eebc8e608fcd685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:01:51.802948Z",
     "start_time": "2024-07-13T03:01:51.787877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "id": "914474bc8c4d76db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the model",
   "id": "38d991cdf7145a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T09:00:27.689598Z",
     "start_time": "2024-07-13T03:01:53.060007Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "47473d537caa3dd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1121' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1120/1120 5:58:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.301600</td>\n",
       "      <td>0.289887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.065977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.049841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.040206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.034990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1120' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1120/1120 5:58:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.301600</td>\n",
       "      <td>0.289887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.065977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.049841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.040206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.034990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1120, training_loss=0.831597021647862, metrics={'train_runtime': 21514.5043, 'train_samples_per_second': 0.833, 'train_steps_per_second': 0.052, 'total_flos': 1.09125253988352e+16, 'train_loss': 0.831597021647862, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "save model  and tokenizer",
   "id": "593fd50ca7af1ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T11:00:44.651543Z",
     "start_time": "2024-07-13T11:00:43.661306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = 't5_data/model'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "id": "3ebbaf7b4b5d86c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_data/model\\\\tokenizer_config.json',\n",
       " 't5_data/model\\\\special_tokens_map.json',\n",
       " 't5_data/model\\\\spiece.model',\n",
       " 't5_data/model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load the model",
   "id": "56354fbb1fa18dd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:18:40.362038Z",
     "start_time": "2024-07-16T11:18:38.968842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = 't5_data/without_eval/model'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ],
   "id": "b8e7143253d51724",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:21:16.928892Z",
     "start_time": "2024-07-16T11:21:16.925728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load test dataset\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n"
   ],
   "id": "f0c0691b1b827825",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:20:04.592826Z",
     "start_time": "2024-07-16T11:20:04.573872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#store the data in an xml file\n",
    "def store_data(data, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "def store_data_in_xml(data, file_name):\n",
    "    # Create the root element\n",
    "    root = ET.Element(\"data\")\n",
    "    \n",
    "    # Iterate through the dictionary, adding each key-value pair as a child element\n",
    "    for key, value in data.items():\n",
    "        child = ET.SubElement(root, key)\n",
    "        child.text = str(value)\n",
    "    \n",
    "    # Create an ElementTree object and write to file\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(file_name, encoding='utf-8', xml_declaration=True)\n",
    "data_to_store = {\n",
    "    'input': test_dataset[0]['output'],\n",
    "    # Add other fields here if needed, for example:\n",
    "    # 'output': test_dataset[0]['output']\n",
    "}\n",
    "store_data_in_xml(data_to_store, \"example_out.xmi\")    "
   ],
   "id": "379b2407ea1456d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m     tree \u001B[38;5;241m=\u001B[39m ET\u001B[38;5;241m.\u001B[39mElementTree(root)\n\u001B[0;32m     20\u001B[0m     tree\u001B[38;5;241m.\u001B[39mwrite(file_name, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m, xml_declaration\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     21\u001B[0m data_to_store \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m: test_dataset[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# Add other fields here if needed, for example:\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# 'output': test_dataset[0]['output']\u001B[39;00m\n\u001B[0;32m     25\u001B[0m }\n\u001B[0;32m     26\u001B[0m store_data_in_xml(data_to_store, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample_out.xmi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inferencing",
   "id": "73360e25fb264dd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:25:14.318646Z",
     "start_time": "2024-07-16T11:25:14.294782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix = \"complete: \"\n",
    "max_length = 512\n",
    "#inputs = [prefix + d for d in test_data['input']]\n",
    "#targets = [d for d in test_data['output']]\n",
    "input = test_dataset[0]['input']\n",
    "\n",
    "inputs = [prefix + input]\n",
    "\n",
    "model_input = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True,  return_tensors=\"pt\")\n",
    "model_input"
   ],
   "id": "623c15082fc09561",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  743,    10,     3,     2,    58,   226,    51,    40,   988,  2423,\n",
       "            31, 12734,    31,     3,    35,  9886,  2423,    31,    76,    17,\n",
       "            89,  6039,    31,    58,  3155,     3,     2,   226,    51,    23,\n",
       "            10,     4,  7075,     3,   226,    51,    40,    29,     7,    10,\n",
       "           440,    40, 17592,  5948,  1303,  1986,     5,    15, 16744,     7,\n",
       "            15,     5,  1677,    87,   440,    40, 15896,  9125, 11739,    87,\n",
       "          6122,   434,   121,     3,   226,    51,    40,    29,     7,    10,\n",
       "           226,    51,    23, 17592,  5948,  1303,     7,  6482,     9,     5,\n",
       "            32,    51,   122,     5,  1677,    87,  7576,    87,     4,  7075,\n",
       "            87, 14489,   121,     3,   226,    51,    40,    29,     7,    10,\n",
       "           226,     7,    23, 17592,  5948,  1303,  1986,     5,   210,  5787,\n",
       "          1677,    87, 23658,    87,     4, 17976,  6482,     9,    18,    77,\n",
       "          8389,   121,     3,   226,    51,    23,    10,  8674, 17592, 14489,\n",
       "           121,  3155,     3,     2,   440,    40,    10, 24663,     3,   226,\n",
       "            51,    23,    10,    23,    26, 17592,   834,   357,  5173,    75,\n",
       "           196,   683,  4630,   427,    15,  1824,  1824,   517,   956,   107,\n",
       "          4448,  5091,    17,     4,  2247,   121,   564, 17592, 21770,   121,\n",
       "          3155,     3,     2,   226,    51,    23,    10,  5420, 13177,  4285,\n",
       "            49, 17592,  5948,  1303,  1986,     5,    15, 16744,     7,    15,\n",
       "             5,  1677,    87,    15,    51,    89,    87, 24898,    87,   427,\n",
       "          9022,   121,  3155,     3,     2,    15, 17608,    32,    17,  1628,\n",
       "             3,   226,    51,    23,    10,    23,    26, 17592,   834,   357,\n",
       "          5173,    75, 20091,  4630,   427,    15,  1824,  1824,   517,   956,\n",
       "           107,  4448,  5091,    17,     4,  2247,   121,  1391, 17592,   729,\n",
       "          2258, 21770,   121,  3155,     3,     2,   221,  5756,     7,     3,\n",
       "           226,    51,    23,    10,    23,    26, 17592,   834,   357,  5173,\n",
       "            75,   196,   102,  4630,   427,    15,  1824,  1824,   517,   956,\n",
       "           107,  4448,  5091,    17,     4,  2247,   121,   843, 17592,    76,\n",
       "            76,    23,    26,   121,   701, 17592,  3072,   115,   591,   658,\n",
       "          1206,    18,     9,   948,  3288,    18,  3628,  2469,    18,   115,\n",
       "           357,   115,  7412,  3710,  3264,    75,  1206,   948,  2517,   115,\n",
       "         20364,     3,    87,  3155,     3,     2,    87,    15, 17608,    32,\n",
       "            17,  1628,  3155,     3,     2,    87,   226,    51,    23,    10,\n",
       "          5420, 13177,  3155,     3,     2,  9160,  5890,   297,     3,   226,\n",
       "            51,    23,    10,    23,    26, 17592,   834,   357,  5173,    75,\n",
       "           196,   755,  4630,   427,    15,  1824,  1824,   517,   956,   107,\n",
       "          4448,  5091,    17,     4,  2247,   121,   643, 17592,   371,  2242,\n",
       "             7,   184,  4663,  1714,   117,    18,  1326,   241,    12,   825,\n",
       "             3,     9,   358,    21,   758,    13,  7534,    11,  4487,     7,\n",
       "             5,   389, 11545, 10291,  7534,     5,  1698, 11545,    65,    46,\n",
       "          4699,     5,   184,  4663,  1714,   117,    18,   427,  1836,  3777,\n",
       "            65,    46,  4699,     3,     9, 12028,  3761,    11,    46,  6870,\n",
       "          3761,    10,    46,  3761,    38,     3,     9,   775,     3,  8826,\n",
       "            52,     5,   184,  4663,  1714,   117,    18,   427,  1836,  3777,\n",
       "            65,     3,     9,  4487,    11,     3,     9,   576,    18, 24650,\n",
       "             6,    11,    34,  2284,    46,  6442,    13,     3,     9,   824,\n",
       "           686,   117,     3,     9,  3777,    65,    92,     3,     9, 12028,\n",
       "            97,    11,    46,  6870,    97,     5,   184,  4663,  1714,   117,\n",
       "            18,   188,    29, 11545,   293,     7,     3,     9,   356,    13,\n",
       "          6442,     7,    13,   315,  1308,     5,     3,   184,  4663,  1714,\n",
       "           117,    18,   188,    29,  6442,    54,    36,    16,     3,     9,\n",
       "           464,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:21:56.327032Z",
     "start_time": "2024-07-16T11:21:34.476334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix = \"complete: \"\n",
    "max_length = 512\n",
    "#inputs = [prefix + d for d in test_data['input']]\n",
    "#targets = [d for d in test_data['output']]\n",
    "input = test_dataset[0]['input']\n",
    "\n",
    "inputs = [prefix + input]\n",
    "\n",
    "model_input = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True,  return_tensors=\"pt\")\n",
    "output = model.generate(**model_input, num_beams=8, do_sample=True, min_length=10, max_length=248)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "data_to_store = {\n",
    "    'input': decoded_output,\n",
    "    # Add other fields here if needed, for example:\n",
    "    # 'output': test_dataset[0]['output']\n",
    "}\n",
    "\n",
    "store_data_in_xml(data_to_store, \"example_out2.xmi\") "
   ],
   "id": "dcf88ceb872bb49d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5dbcf76107bf55d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
