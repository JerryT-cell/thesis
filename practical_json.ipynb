{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:51.420712Z",
     "start_time": "2024-08-13T11:19:51.417228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os"
   ],
   "id": "2df2de4a6951e750",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:13:15.459616Z",
     "start_time": "2024-08-10T23:13:15.448046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#to login for token\n",
    "from huggingface_hub import login\n",
    "login()"
   ],
   "id": "3320a89aa4b039aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67863cad05fe4bf296246dded9a46b87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the tokenizer to tokenize the text data",
   "id": "5d8a7eb9c1d940f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:53.368311Z",
     "start_time": "2024-08-13T11:19:52.783626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"google/gemma-2-2b\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ],
   "id": "c246a56962c6b21b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:53.383633Z",
     "start_time": "2024-08-13T11:19:53.369967Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ea7b4896a7111a76",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KNOW THE DATA : \n",
    "### 1. Do a statistical analysis of the data. \n",
    "### 2. Figure out how many tokens are used for non-important data"
   ],
   "id": "63e001dce630e5c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Statistical Analysis\n",
    "Counts the number of tokens in a string"
   ],
   "id": "16094be54459ee97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:53.531246Z",
     "start_time": "2024-08-13T11:19:53.529689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using the provided tokenizer.\n",
    "\n",
    "    :param text: The input text to tokenize.\n",
    "    :return: The number of tokens in the text.\n",
    "    \"\"\"\n",
    "    encoded_tokens = tokenizer.encode(text)\n",
    "    return len(encoded_tokens)"
   ],
   "id": "2fa5adda5d49d5d7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "counts the number of tokens in a Json file",
   "id": "885a3d98e1135ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:53.942156Z",
     "start_time": "2024-08-13T11:19:53.940120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def count_tokens_in_file(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Reads the content of the file and returns the number of tokens using the provided tokenizer.\n",
    "\n",
    "    :param file_path: The path to the file.\n",
    "    :return: The number of tokens in the file content.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return count_tokens(content)"
   ],
   "id": "e5f32a55b0a62949",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Counts the number of tokens in a folder and make a statistics with categories",
   "id": "b5e90459e37b3232"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:54.407680Z",
     "start_time": "2024-08-13T11:19:54.405883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_files_in_folder(folder_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of files in a folder.\n",
    "\n",
    "    :param folder_path: The path to the folder.\n",
    "    :return: The number of files in the folder.\n",
    "    \"\"\"\n",
    "    file_count = 0\n",
    "    for _ in os.listdir(folder_path):\n",
    "        file_count += 1\n",
    "    return file_count"
   ],
   "id": "83d57a63217fdeca",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count the number of tokens in a folder full of folders of JSON files",
   "id": "946dea49960bd219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:54.899214Z",
     "start_time": "2024-08-13T11:19:54.896126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens_folder_in_folder(folder_path):\n",
    "\n",
    "    token_counts = {\n",
    "        '<1000': 0,\n",
    "        '<2000': 0,\n",
    "        '<3000': 0,\n",
    "        '<4000': 0,\n",
    "        '<5000': 0,\n",
    "        '<6000': 0,\n",
    "        '<7000': 0,\n",
    "        '<=8192': 0,\n",
    "        '>8192': 0\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for folder_name in os.listdir(base_folder_path):\n",
    "        folder_path = os.path.join(base_folder_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            new_file = f\"{folder_name.replace('.xmi','')}.json\"\n",
    "            json_file_path = os.path.join(folder_path, new_file)\n",
    "            if os.path.isfile(json_file_path):\n",
    "               \n",
    "                num_tokens = count_tokens_in_file(json_file_path)\n",
    "                if num_tokens < 1000:\n",
    "                    token_counts['<1000'] += 1\n",
    "                elif num_tokens < 2000:\n",
    "                    token_counts['<2000'] += 1\n",
    "                elif num_tokens < 3000:\n",
    "                    token_counts['<3000'] += 1\n",
    "                elif num_tokens < 4000:\n",
    "                    token_counts['<4000'] += 1\n",
    "                elif num_tokens < 5000:\n",
    "                    token_counts['<5000'] += 1\n",
    "                elif num_tokens < 6000:\n",
    "                    token_counts['<6000'] += 1\n",
    "                elif num_tokens < 7000:\n",
    "                     token_counts['<7000'] += 1\n",
    "                elif num_tokens <= 8192:\n",
    "                     token_counts['<=8192'] += 1\n",
    "                elif num_tokens > 8192:\n",
    "                     token_counts['>8192'] += 1\n",
    "\n",
    "\n",
    "    for category, count in token_counts.items():\n",
    "        print(f\"Number of json files with tokens {category}: {count}\")\n"
   ],
   "id": "baef402b3837239",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Execution !!!",
   "id": "557c33f2ff7ccff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:20:29.985944Z",
     "start_time": "2024-08-13T11:19:55.551294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_folder_path = 'modelset/graph/repo-genmymodel-uml/data'\n",
    "numbers = count_files_in_folder(base_folder_path)\n",
    "print(f\"There are {numbers} files in the folder.\")\n",
    "count_tokens_folder_in_folder(base_folder_path)"
   ],
   "id": "c00cdd7d76b8cc77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5120 files in the folder.\n",
      "Number of json files with tokens <1000: 0\n",
      "Number of json files with tokens <2000: 132\n",
      "Number of json files with tokens <3000: 279\n",
      "Number of json files with tokens <4000: 315\n",
      "Number of json files with tokens <5000: 301\n",
      "Number of json files with tokens <6000: 520\n",
      "Number of json files with tokens <7000: 358\n",
      "Number of json files with tokens <=8192: 636\n",
      "Number of json files with tokens >8192: 2579\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e61f76c43c5d3a2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
