{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:03.232171Z",
     "start_time": "2024-11-24T11:56:59.759880Z"
    }
   },
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
    "from accelerate import Accelerator"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:05.350715Z",
     "start_time": "2024-11-24T11:57:05.334624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    Arguments for the fine_tuning\n",
    "    \"\"\"\n",
    "    base_model = \"google/gemma-2-2b-it\" \n",
    "    fine_tuned_model = \"gemma-2-2b-it-software-model_completion_finetuned\"\n",
    "    merged_model = \"gemma-2-2b-it-software-model_completion\"\n",
    "    dataset_name = \"/Users/jerrytakou/University/Thesis/programming/thesis\"\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=4)\n",
    "    evaluation_strategy: Optional[str] = field(default=\"steps\")\n",
    "    evaluation_accumulation_steps: Optional[int] = field(default=5)\n",
    "    learning_rate: Optional[float] = field(default=2e-4)\n",
    "    max_grad_norm: Optional[float] = field(default=0.3)\n",
    "    weight_decay: Optional[int] = field(default=0.001)\n",
    "    lora_alpha= 64,\n",
    "    lora_dropout =  0.5,\n",
    "    lora_r = 32\n",
    "    max_seq_length: Optional[int] = field(default=4100)\n",
    "    fp16 = True\n",
    "    bf16 = False\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    use_flash_attention_2: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables Flash Attention 2.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
    "    )\n",
    "    max_steps: int = field(default=100, metadata={\"help\": \"How many optimizer update steps to take\"}),\n",
    "    epochs : int = field(default=3, metadata={\"help\": \"How many epochs to train for\"})\n",
    "    warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n",
    "    save_steps: int = field(default=87, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    logging_steps: int = field(default=87, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    output_dir: str = field(\n",
    "        default=\"./gemma/results\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    logging_dir: str = field(\n",
    "        default=\"./gemma-2b/logs\",\n",
    "        metadata={\"help\": \"The output directory where the logs will be written.\"},\n",
    "    )\n",
    "    eval_steps: int = field(default=87, metadata={\"help\": \"How often to evaluate the model\"})\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "# Parse the arguments, ignoring unrecognized ones\n",
    "script_args, remaining_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)"
   ],
   "id": "eac753e205fde1aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:07.663525Z",
     "start_time": "2024-11-24T11:57:07.648409Z"
    }
   },
   "cell_type": "code",
   "source": "access_token = \"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\"",
   "id": "8c0c7147713166cf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:08.612363Z",
     "start_time": "2024-11-24T11:57:08.601847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ],
   "id": "eb5ce2a848cebb48",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:17.466177Z",
     "start_time": "2024-11-24T11:57:10.001516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)"
   ],
   "id": "b2f3c115bb144a91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8dbbf19cdeb94b958917a53b3d0f8898"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ],
   "id": "e9cb8a40d43c9d15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:17.472954Z",
     "start_time": "2024-11-24T11:57:17.466177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ],
   "id": "8349660f34e7ce4e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:17.842942Z",
     "start_time": "2024-11-24T11:57:17.499216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the model for kbit training\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "2e3bdc45de2d2567",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:19.173208Z",
     "start_time": "2024-11-24T11:57:18.534383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the dataset\n",
    "\n",
    "#windows\n",
    "# Load dataset\n",
    "'''org_path = \"D:\\LLM\\\\thesisPractical\\\\datasets_for_fine_tuning\\\\structural_removal_non_contiguous\\\\processed_2000\"\n",
    "\n",
    "train_dataset_url = org_path + \"\\\\train.jsonl\"\n",
    "test_dataset_url =org_path + \"\\\\test.jsonl\"\n",
    "validation_dataset_url =org_path + \"\\\\validation.jsonl\"'''\n",
    "\n",
    "#linux/abs\n",
    "\n",
    "abs_path = script_args.dataset_name\n",
    "dataset_to_use = \"processed_4000\"\n",
    "train_dataset_url = f\"{abs_path}/{dataset_to_use}/train.jsonl\"\n",
    "test_dataset_url = f\"{abs_path}/{dataset_to_use}/test.jsonl\"\n",
    "validation_dataset_url = f\"{abs_path}/{dataset_to_use}/validation.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'train': train_dataset_url,\n",
    "    'test': test_dataset_url,\n",
    "    'validation': validation_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ],
   "id": "89d35cfa26177251",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:21.408885Z",
     "start_time": "2024-11-24T11:57:20.697269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transform the data\n",
    "instruction = \"\"\"You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model : \"\"\"\n",
    "\n",
    "def format_chat_template2(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": f'You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model :\\n{row[\"input\"]}'},\n",
    "        {\"role\": \"model\", \"content\": row[\"output\"]}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "trained_data = train_dataset.map(format_chat_template2)\n",
    "validation_data = validation_dataset.map(format_chat_template2)\n",
    "test_data = test_dataset.map(format_chat_template2)\n",
    "\n",
    "print(trained_data['text'][0])"
   ],
   "id": "815556e1e8cf1290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model :\n",
      "{\"directed\":true,\"nodes\":[{\"viewpoint\":null,\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"model\",\"name\":\"model\",\"id\":0,\"URI\":null,\"eClass\":\"Model\"},{\"visibility\":\"PUBLIC_LITERAL\",\"id\":1,\"eClass\":\"PackageImport\"},{\"isSingleExecution\":false,\"isReadOnly\":false,\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"model::Activity\",\"name\":\"Activity\",\"id\":2,\"isActive\":false,\"isReentrant\":true,\"isLeaf\":false,\"isAbstract\":false,\"isFinalSpecialization\":false,\"eClass\":\"Activity\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":3,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":4,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":5,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":7,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":8,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":9,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":10,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":12,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"InitialNode\",\"name\":\"InitialNode\",\"id\":13,\"isLeaf\":false,\"eClass\":\"InitialNode\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"click on link of form \",\"isLocallyReentrant\":false,\"name\":\"click on link of form \",\"language\":[],\"id\":14,\"body\":[],\"isLeaf\":false,\"eClass\":\"OpaqueAction\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"DecisionNode\",\"name\":\"DecisionNode\",\"id\":15,\"isLeaf\":false,\"eClass\":\"DecisionNode\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"OpaqueAction\",\"isLocallyReentrant\":false,\"name\":\"OpaqueAction\",\"language\":[],\"id\":16,\"body\":[],\"isLeaf\":false,\"eClass\":\"OpaqueAction\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"DecisionNode2\",\"name\":\"DecisionNode2\",\"id\":17,\"isLeaf\":false,\"eClass\":\"DecisionNode\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"OpaqueAction2\",\"isLocallyReentrant\":false,\"name\":\"OpaqueAction2\",\"language\":[],\"id\":18,\"body\":[],\"isLeaf\":false,\"eClass\":\"OpaqueAction\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"OpaqueAction3\",\"isLocallyReentrant\":false,\"name\":\"OpaqueAction3\",\"language\":[],\"id\":19,\"body\":[],\"isLeaf\":false,\"eClass\":\"OpaqueAction\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"MergeNode\",\"name\":\"MergeNode\",\"id\":20,\"isLeaf\":false,\"eClass\":\"MergeNode\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ActivityFinalNode\",\"name\":\"ActivityFinalNode\",\"id\":21,\"isLeaf\":false,\"eClass\":\"ActivityFinalNode\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":24,\"value\":\"\\u0629  \",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":26,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":27,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":28,\"value\":\"true\",\"eClass\":\"LiteralString\"}],\"links\":[{\"source\":0,\"target\":1},{\"source\":0,\"target\":2},{\"source\":1,\"target\":0},{\"source\":2,\"target\":3},{\"source\":2,\"target\":4},{\"source\":2,\"target\":5},{\"source\":2,\"target\":7},{\"source\":2,\"target\":8},{\"source\":2,\"target\":9},{\"source\":2,\"target\":10},{\"source\":2,\"target\":12},{\"source\":2,\"target\":13},{\"source\":2,\"target\":14},{\"source\":2,\"target\":15},{\"source\":2,\"target\":16},{\"source\":2,\"target\":17},{\"source\":2,\"target\":18},{\"source\":2,\"target\":19},{\"source\":2,\"target\":20},{\"source\":2,\"target\":21},{\"source\":3,\"target\":2},{\"source\":3,\"target\":14},{\"source\":3,\"target\":13},{\"source\":4,\"target\":2},{\"source\":4,\"target\":16},{\"source\":4,\"target\":15},{\"source\":5,\"target\":2},{\"source\":5,\"target\":24},{\"source\":5,\"target\":17},{\"source\":5,\"target\":15},{\"source\":7,\"target\":2},{\"source\":7,\"target\":26},{\"source\":7,\"target\":19},{\"source\":7,\"target\":17},{\"source\":8,\"target\":2},{\"source\":8,\"target\":27},{\"source\":8,\"target\":18},{\"source\":8,\"target\":17},{\"source\":9,\"target\":2},{\"source\":9,\"target\":28},{\"source\":9,\"target\":20},{\"source\":9,\"target\":18},{\"source\":10,\"target\":2},{\"source\":10,\"target\":20},{\"source\":10,\"target\":19},{\"source\":12,\"target\":2},{\"source\":12,\"target\":21},{\"source\":12,\"target\":16},{\"source\":13,\"target\":3},{\"source\":14,\"target\":3},{\"source\":15,\"target\":4},{\"source\":15,\"target\":5},{\"source\":16,\"target\":4},{\"source\":16,\"target\":12},{\"source\":17,\"target\":5},{\"source\":17,\"target\":7},{\"source\":17,\"target\":8},{\"source\":18,\"target\":8},{\"source\":18,\"target\":9},{\"source\":19,\"target\":7},{\"source\":19,\"target\":10},{\"source\":20,\"target\":9},{\"source\":20,\"target\":10},{\"source\":21,\"target\":12}],\"multigraph\":true}<|im_end|>\n",
      "<|im_start|>model\n",
      "{\"nodes\":[{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":6,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":\"ControlFlow\",\"name\":\"ControlFlow\",\"id\":11,\"isLeaf\":false,\"eClass\":\"ControlFlow\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":22,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":23,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":29,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":31,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":25,\"value\":\"true\",\"eClass\":\"LiteralString\"},{\"visibility\":\"PUBLIC_LITERAL\",\"qualifiedName\":null,\"name\":null,\"id\":30,\"value\":\"true\",\"eClass\":\"LiteralString\"}],\"links\":[{\"source\":2,\"target\":6},{\"source\":2,\"target\":11},{\"source\":3,\"target\":22},{\"source\":4,\"target\":23},{\"source\":6,\"target\":2},{\"source\":6,\"target\":25},{\"source\":6,\"target\":15},{\"source\":6,\"target\":14},{\"source\":10,\"target\":29},{\"source\":11,\"target\":2},{\"source\":11,\"target\":30},{\"source\":11,\"target\":21},{\"source\":11,\"target\":20},{\"source\":12,\"target\":31},{\"source\":14,\"target\":6},{\"source\":15,\"target\":6},{\"source\":20,\"target\":11},{\"source\":21,\"target\":11}]}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:23.395091Z",
     "start_time": "2024-11-24T11:57:23.345316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    save_steps=script_args.save_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    optim=script_args.optim,\n",
    "    num_train_epochs=script_args.epochs,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    eval_strategy=script_args.evaluation_strategy,\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    eval_accumulation_steps=script_args.evaluation_accumulation_steps,\n",
    "    logging_dir=script_args.logging_dir,\n",
    "    warmup_ratio=script_args.warmup_ratio,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    max_seq_length= script_args.max_seq_length,\n",
    "    fp16=script_args.fp16,\n",
    "    bf16=script_args.bf16,\n",
    "\n",
    ")"
   ],
   "id": "25b7595d611045fa",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:25.802295Z",
     "start_time": "2024-11-24T11:57:24.963013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#train\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=trained_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=script_args.max_seq_length,\n",
    "    dataset_text_field=\"text\"\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ],
   "id": "880488374e29731f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56ff10577056483fb03565c190bd17f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:00:16.333641Z",
     "start_time": "2024-11-24T11:57:27.777724Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "58e2febe62973fe7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 02:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.725100</td>\n",
       "      <td>0.731002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.155829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.111164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.065271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.050339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26, training_loss=0.5180104386347991, metrics={'train_runtime': 167.8882, 'train_samples_per_second': 0.625, 'train_steps_per_second': 0.155, 'total_flos': 127625759539200.0, 'train_loss': 0.5180104386347991, 'epoch': 0.9904761904761905})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving the Model !",
   "id": "9896f2bf7961b20a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:39.895701Z",
     "start_time": "2024-11-24T12:56:32.316660Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.model.save_pretrained(script_args.fine_tuned_model)",
   "id": "4e085ee32e205e36",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\peft\\utils\\save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:26:32.858733Z",
     "start_time": "2024-11-24T13:26:25.744834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ],
   "id": "d4bc74e2d318a6f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "255b2020d09f42dcb787cdce56d24cde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:29:13.248025Z",
     "start_time": "2024-11-24T13:28:33.425748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "model = PeftModel.from_pretrained(base_model_reload, script_args.fine_tuned_model)\n",
    "\n",
    "model = model.merge_and_unload()"
   ],
   "id": "2f1be5d08b238865",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:32:15.459383Z",
     "start_time": "2024-11-24T13:32:02.905762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(script_args.merged_model)\n",
    "tokenizer.save_pretrained(script_args.merged_model)"
   ],
   "id": "ef8f6f9ea883081f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-2b_model_test_merged\\\\tokenizer_config.json',\n",
       " 'gemma-2b_model_test_merged\\\\special_tokens_map.json',\n",
       " 'gemma-2b_model_test_merged\\\\tokenizer.model',\n",
       " 'gemma-2b_model_test_merged\\\\added_tokens.json',\n",
       " 'gemma-2b_model_test_merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "2a1b774f4f30109d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = dataset[\"test\"]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ],
   "id": "7ae2bf916d50be24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
