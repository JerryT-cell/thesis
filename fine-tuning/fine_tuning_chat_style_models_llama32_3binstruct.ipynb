{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning Chat Style Model for UML Model Completion\n",
    "\n",
    "This scripts fine-tunes the llama3.2-3b model for uml model completion"
   ],
   "id": "2acc5677d2f3baab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:40:12.731648Z",
     "start_time": "2024-11-21T08:40:09.531967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
    "from accelerate import Accelerator"
   ],
   "id": "cc81844b0023ba68",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The fine-tuning script requires the following arguments:",
   "id": "d2ddd15092660d69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:40:14.034187Z",
     "start_time": "2024-11-21T08:40:14.025571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    Arguments for the fine_tuning\n",
    "    \"\"\"\n",
    "    base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    fine_tuned_model = \"Llama-3.2-3B-Instruct-software-model_completion_finetuned\"\n",
    "    merged_model = \"Llama-3.2-3B-Instruct-software-model_completion\"\n",
    "    dataset_name = \"/Users/jerrytakou/University/Thesis/programming/thesis\"\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=4)\n",
    "    evaluation_strategy: Optional[str] = field(default=\"steps\")\n",
    "    evaluation_accumulation_steps: Optional[int] = field(default=5)\n",
    "    learning_rate: Optional[float] = field(default=2e-4)\n",
    "    max_grad_norm: Optional[float] = field(default=0.3)\n",
    "    weight_decay: Optional[int] = field(default=0.001)\n",
    "    lora_alpha= 64,\n",
    "    lora_dropout =  0.5,\n",
    "    lora_r = 32\n",
    "    max_seq_length: Optional[int] = field(default=4100)\n",
    "    fp16 = True\n",
    "    bf16 = False\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    use_flash_attention_2: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables Flash Attention 2.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
    "    )\n",
    "    max_steps: int = field(default=100, metadata={\"help\": \"How many optimizer update steps to take\"}),\n",
    "    epochs : int = field(default=3, metadata={\"help\": \"How many epochs to train for\"})\n",
    "    warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n",
    "    save_steps: int = field(default=87, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    logging_steps: int = field(default=87, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    output_dir: str = field(\n",
    "        default=\"./llama32_3b_instruct/results\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    logging_dir: str = field(\n",
    "        default=\"./llama32_3b_instruct/logs\",\n",
    "        metadata={\"help\": \"The output directory where the logs will be written.\"},\n",
    "    )\n",
    "    eval_steps: int = field(default=87, metadata={\"help\": \"How often to evaluate the model\"})\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "# Parse the arguments, ignoring unrecognized ones\n",
    "script_args, remaining_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)"
   ],
   "id": "eac753e205fde1aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "My Hugging face access",
   "id": "5948493f2a71d879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "access_token = \"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\"",
   "id": "8c0c7147713166cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:40:21.932533Z",
     "start_time": "2024-11-21T08:40:21.928202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ],
   "id": "eb5ce2a848cebb48",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:50:06.512256Z",
     "start_time": "2024-11-21T08:40:23.910038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)\n"
   ],
   "id": "b2f3c115bb144a91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ae4d9b17f784ab6ad13fd742e2eba1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8f98ba273534eccbac8596acc01329f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c34ac2c0be246a59d5f206c6fa72b7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f238247e11c5454fab0c11696d2ef154"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb6dc430ae4042ee8e6f2d7ebeec3a35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93a0d60bb1154847abb737b108c5c572"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f8b0a8fd0b94dff97e53e5e51a4696a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2199c0da82ee4d3d9bda93ae37da4d5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d302de638384172a95f58feb6fccc5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cddac9a648449738889c3fd16a2621e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ],
   "id": "1b7e160144b61a36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ],
   "id": "35aa2b7f2ffc7e48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the model for kbit training\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "2e3bdc45de2d2567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load the dataset\n",
    "abs_path = script_args.dataset_name\n",
    "dataset_to_use = \"processed_4000\"\n",
    "train_dataset_url = f\"{abs_path}/{dataset_to_use}/train.jsonl\"\n",
    "test_dataset_url = f\"{abs_path}/{dataset_to_use}/test.jsonl\"\n",
    "validation_dataset_url = f\"{abs_path}/{dataset_to_use}/validation.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'train': train_dataset_url,\n",
    "    'test': test_dataset_url,\n",
    "    'validation': validation_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ],
   "id": "89d35cfa26177251"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# transform the data\n",
    "instruction = \"You are an AI assistant that specializes in UML model completion. Given an incomplete UML model represented in JSON format, output the missing portions of the model in JSON format.\"\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the incomplete UML model:\\n{row[\"input\"]}'},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"output\"]}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "trained_data = train_dataset.map(format_chat_template)\n",
    "validation_data = validation_dataset.map(format_chat_template)\n",
    "test_data = test_dataset.map(format_chat_template)"
   ],
   "id": "815556e1e8cf1290"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    save_steps=script_args.save_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    optim=script_args.optim,\n",
    "    num_train_epochs=script_args.epochs,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    eval_strategy=script_args.evaluation_strategy,\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    eval_accumulation_steps=script_args.evaluation_accumulation_steps,\n",
    "    logging_dir=script_args.logging_dir,\n",
    "    warmup_ratio=script_args.warmup_ratio,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    max_seq_length= script_args.max_seq_length,\n",
    "    fp16=script_args.fp16,\n",
    "    bf16=script_args.bf16,\n",
    "\n",
    ")"
   ],
   "id": "25b7595d611045fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#train\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=trained_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=script_args.max_seq_length,\n",
    "    dataset_text_field=\"text\"\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ],
   "id": "880488374e29731f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "58e2febe62973fe7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving the Model !",
   "id": "e0dca9bfe83c750c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.model.save_pretrained(script_args.fine_tuned_model)",
   "id": "e2a3bdbfd46bc511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ],
   "id": "14765d8d4c81d749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, script_args.fine_tuned_model)\n",
    "\n",
    "model = model.merge_and_unload()"
   ],
   "id": "d1a8ba156cee4c36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save_pretrained(script_args.merged_model)\n",
    "tokenizer.save_pretrained(script_args.merged_model)"
   ],
   "id": "d4dec6d2cbd124f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "b8182661858c8145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cde2e4bf9616ef62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
