{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:11:39.293880Z",
     "start_time": "2024-10-08T03:11:37.133823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os"
   ],
   "id": "2df2de4a6951e750",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:11:39.446473Z",
     "start_time": "2024-10-08T03:11:39.407395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#to login for token\n",
    "from huggingface_hub import login\n",
    "login()"
   ],
   "id": "3320a89aa4b039aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30dc7734aada427a90c2c694cf0676d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the tokenizer to tokenize the text data",
   "id": "5d8a7eb9c1d940f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:11:42.373003Z",
     "start_time": "2024-10-08T03:11:41.774326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model_id = \"google/gemma-2-2b\"\n",
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "llama = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "google = \"google/gemma-2-2b\"\n",
    "\n",
    "model_id = llama\n",
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "id": "c246a56962c6b21b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:19:53.383633Z",
     "start_time": "2024-08-13T11:19:53.369967Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ea7b4896a7111a76",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KNOW THE DATA : \n",
    "### 1. Do a statistical analysis of the data. \n",
    "### 2. Figure out how many tokens are used for non-important data"
   ],
   "id": "63e001dce630e5c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Statistical Analysis\n",
    "Counts the number of tokens in a string"
   ],
   "id": "16094be54459ee97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:11:44.248418Z",
     "start_time": "2024-10-08T03:11:44.245803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using the provided tokenizer.\n",
    "\n",
    "    :param text: The input text to tokenize.\n",
    "    :return: The number of tokens in the text.\n",
    "    \"\"\"\n",
    "    encoded_tokens = tokenizer.encode(text)\n",
    "    return len(encoded_tokens)"
   ],
   "id": "2fa5adda5d49d5d7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "counts the number of tokens in a Json file",
   "id": "885a3d98e1135ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:11:46.210260Z",
     "start_time": "2024-10-08T03:11:46.207282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def count_tokens_in_file(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Reads the content of the file and returns the number of tokens using the provided tokenizer.\n",
    "\n",
    "    :param file_path: The path to the file.\n",
    "    :return: The number of tokens in the file content.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return count_tokens(content)"
   ],
   "id": "e5f32a55b0a62949",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Counts the number of tokens in a folder and make a statistics with categories",
   "id": "b5e90459e37b3232"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:11:47.947521Z",
     "start_time": "2024-10-08T03:11:47.941815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_files_in_folder(folder_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of files in a folder.\n",
    "\n",
    "    :param folder_path: The path to the folder.\n",
    "    :return: The number of files in the folder.\n",
    "    \"\"\"\n",
    "    file_count = 0\n",
    "    for _ in os.listdir(folder_path):\n",
    "        file_count += 1\n",
    "    return file_count"
   ],
   "id": "83d57a63217fdeca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count the number of tokens in a folder full of folders of JSON files",
   "id": "946dea49960bd219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:12:17.133439Z",
     "start_time": "2024-10-08T03:12:17.128105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens_folder_in_folder(folder_path):\n",
    "\n",
    "    token_counts = {\n",
    "        '<1000': 0,\n",
    "        '<2000': 0,\n",
    "        '<3000': 0,\n",
    "        '<4000': 0,\n",
    "        '<5000': 0,\n",
    "        '<6000': 0,\n",
    "        '<7000': 0,\n",
    "        '<8000': 0,\n",
    "        '<9000': 0,\n",
    "        '<10000' :0,\n",
    "        '<11000' :0,\n",
    "        '<12000' :0,\n",
    "        '<13000' :0,\n",
    "        '<14000' :0,\n",
    "        '<15000' :0,\n",
    "        '<20000' :0,\n",
    "        '>20000' :0\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for folder_name in os.listdir(base_folder_path):\n",
    "        folder_path = os.path.join(base_folder_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            new_file = f\"{folder_name.replace('.xmi','')}.json\"\n",
    "            json_file_path = os.path.join(folder_path, new_file)\n",
    "            if os.path.isfile(json_file_path):\n",
    "               \n",
    "                num_tokens = count_tokens_in_file(json_file_path)\n",
    "                if num_tokens < 1000:\n",
    "                     token_counts['<1000'] += 1\n",
    "                elif num_tokens < 2000:\n",
    "                     token_counts['<2000'] += 1\n",
    "                elif num_tokens < 3000:\n",
    "                     token_counts['<3000'] += 1\n",
    "                elif num_tokens < 4000:\n",
    "                     token_counts['<4000'] += 1\n",
    "                elif num_tokens < 5000:\n",
    "                     token_counts['<5000'] += 1\n",
    "                elif num_tokens < 6000:\n",
    "                     token_counts['<6000'] += 1\n",
    "                elif num_tokens < 7000:\n",
    "                     token_counts['<7000'] += 1\n",
    "                elif num_tokens < 8000:\n",
    "                     token_counts['<8000'] += 1\n",
    "                elif num_tokens < 9000:\n",
    "                     token_counts['<9000'] += 1\n",
    "                elif num_tokens < 10000:\n",
    "                     token_counts['<10000'] += 1\n",
    "                elif num_tokens < 11000:\n",
    "                     token_counts['<11000'] += 1\n",
    "                elif num_tokens < 12000:\n",
    "                     token_counts['<12000'] += 1\n",
    "                elif num_tokens < 13000:\n",
    "                     token_counts['<13000'] += 1\n",
    "                elif num_tokens < 14000:\n",
    "                     token_counts['<14000'] += 1\n",
    "                elif num_tokens < 15000:\n",
    "                     token_counts['<15000'] += 1\n",
    "                elif num_tokens < 20000:\n",
    "                     token_counts['<20000'] += 1\n",
    "                else:\n",
    "                     token_counts['>20000'] += 1\n",
    "\n",
    "\n",
    "    for category, count in token_counts.items():\n",
    "        print(f\"Number of json files with tokens {category}: {count}\")\n"
   ],
   "id": "baef402b3837239",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Execution !!!",
   "id": "557c33f2ff7ccff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:13:29.930495Z",
     "start_time": "2024-10-08T03:12:19.302944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_folder_path = 'modelset/graph/repo-genmymodel-uml/data'\n",
    "numbers = count_files_in_folder(base_folder_path)\n",
    "print(f\"There are {numbers} files in the folder.\")\n",
    "count_tokens_folder_in_folder(base_folder_path)"
   ],
   "id": "c00cdd7d76b8cc77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5120 files in the folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (186293 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of json files with tokens <1000: 0\n",
      "Number of json files with tokens <2000: 184\n",
      "Number of json files with tokens <3000: 317\n",
      "Number of json files with tokens <4000: 381\n",
      "Number of json files with tokens <5000: 485\n",
      "Number of json files with tokens <6000: 482\n",
      "Number of json files with tokens <7000: 602\n",
      "Number of json files with tokens <8000: 273\n",
      "Number of json files with tokens <9000: 261\n",
      "Number of json files with tokens <10000: 295\n",
      "Number of json files with tokens <11000: 186\n",
      "Number of json files with tokens <12000: 234\n",
      "Number of json files with tokens <13000: 107\n",
      "Number of json files with tokens <14000: 266\n",
      "Number of json files with tokens <15000: 275\n",
      "Number of json files with tokens <20000: 387\n",
      "Number of json files with tokens >20000: 385\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e61f76c43c5d3a2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
