{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wandb will be done later",
   "id": "5821b9c54bc8063"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "access_token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# QLoRA config\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ],
   "id": "971bc197ea24f8c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "5247f447282f0c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Datasets preparation",
   "id": "1d752869d82ccf4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset_url = \"datasets_train_valid_test/test.jsonl\"\n",
    "test_dataset_url =\"datasets_train_valid_test/test.jsonl\"\n",
    "validation_dataset_url =\"datasets_train_valid_test/test.jsonl\""
   ],
   "id": "77bd963076218c38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Datasets loading",
   "id": "855a5b65ce277a41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_files = {\n",
    "    'train': train_dataset_url,\n",
    "    'test': test_dataset_url,\n",
    "    'validation': validation_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ],
   "id": "1c1dc761acc84d8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Datasets tokenization",
   "id": "524c4f8dd3385b3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['output']\n",
    "    max_length = 2048\n",
    "    model_input = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_input['labels'] = labels['input_ids']\n",
    "    return model_input\n",
    "\n",
    "trained_data = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_data = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_data = test_dataset.map(tokenize_function, batched=True)"
   ],
   "id": "b3c3fab4af283118"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training arguments",
   "id": "7d490b9223bd81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 5\n",
    "epochs = 5\n",
    "max_length = 512\n",
    "output_dir = 't5_datasets_class1/results'\n",
    "logs_dir = 't5_datasets_class1/logs'\n",
    "\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    logging_dir=logs_dir,\n",
    "    warmup_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    ")"
   ],
   "id": "7c6d1a0af6cbe0b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluation metrics",
   "id": "f2927cfa765a89aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "def compute_metrics(eval_pred):\n",
    "    metrics, labels = eval_pred\n",
    "    predictions = np.argmax(metrics, axis=-1)\n",
    "\n",
    "    return perplexity.compute(predictions=predictions, model_id='Meta-Llama-3.1-8B-Instruct')\n"
   ],
   "id": "de8025404d43a7b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training",
   "id": "93e8096e5d0548cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "7b23a5420a85c808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "68f53c664c78e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "saving the model",
   "id": "57c37a2e633b163d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_path = 't5_data/model'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "id": "8b6b57b5a75e834d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
