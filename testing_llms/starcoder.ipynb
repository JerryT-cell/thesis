{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This script is there to go through the starcoder model and see how it works.",
   "id": "156dd67fecff98c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start by importing the necessary libraries.",
   "id": "d23c3afecbc43e8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T12:34:56.798974Z",
     "start_time": "2024-11-19T12:34:53.201678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.classify.util import attested_labels\n",
    "from scipy.signal import max_len_seq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Starcoder2ForCausalLM, Qwen2ForCausalLM"
   ],
   "id": "4fe09365c9cf4c29",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "get the model and tokenizer from the checkpoint.",
   "id": "fc8009f7316074d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T12:34:57.025601Z",
     "start_time": "2024-11-19T12:34:57.023125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "device = \"cuda\""
   ],
   "id": "1bae35c7d52292dc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:05:36.237859Z",
     "start_time": "2024-11-19T15:05:04.719136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ],
   "id": "31e755d7f9af815e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.starcoder2.modeling_starcoder2.Starcoder2ForCausalLM"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:17:49.799316Z",
     "start_time": "2024-11-19T16:17:49.795021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed = model.get_input_embeddings()\n",
    "embed"
   ],
   "id": "2fcdddc96b0b38a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(49152, 3072)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the model with a simple input.",
   "id": "15094c93541ff3ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:27:44.306166Z",
     "start_time": "2024-11-19T16:27:44.290254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(inputs)\n"
   ],
   "id": "386aa4c5ef708e8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   77,   929,    49,   904,   863,   476,   564,  2424,    68,  5491,\n",
      "           863, 12467,   391,   616,    68]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:47:35.146952Z",
     "start_time": "2024-11-19T18:47:35.093483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generate_ids = model.generate(inputs=inputs[\"input_ids\"], max_length=100)\n",
    "\n",
    "print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n"
   ],
   "id": "742f916c9c8e3fdf",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['do'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m generate_ids \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(inputs\u001B[38;5;241m=\u001B[39minputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m], max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, do\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(generate_ids, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\generation\\utils.py:1677\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1675\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001B[39;00m\n\u001B[0;32m   1676\u001B[0m generation_config, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_generation_config(generation_config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1677\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_model_kwargs(model_kwargs\u001B[38;5;241m.\u001B[39mcopy())\n\u001B[0;32m   1678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_assistant(assistant_model)\n\u001B[0;32m   1680\u001B[0m \u001B[38;5;66;03m# 2. Set generation parameters if not already defined\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\generation\\utils.py:1248\u001B[0m, in \u001B[0;36mGenerationMixin._validate_model_kwargs\u001B[1;34m(self, model_kwargs)\u001B[0m\n\u001B[0;32m   1245\u001B[0m         unused_model_args\u001B[38;5;241m.\u001B[39mappend(key)\n\u001B[0;32m   1247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m unused_model_args:\n\u001B[1;32m-> 1248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1249\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following `model_kwargs` are not used by the model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00munused_model_args\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (note: typos in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1250\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m generate arguments will also show up in this list)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1251\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The following `model_kwargs` are not used by the model: ['do'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3983aa4d579f52ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
