{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T08:18:03.072175Z",
     "start_time": "2024-11-21T08:18:01.661626Z"
    }
   },
   "source": [
    "import torch\n",
    "from prompt_toolkit.contrib.regular_languages.regex_parser import tokenize_regex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### Running the model on a GPU",
   "id": "b1e2d643017d2be7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:18:03.546342Z",
     "start_time": "2024-11-21T08:18:03.543378Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = \"google/gemma-2-2b-it\"  ",
   "id": "8998b14db01c4f24",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:18:15.114003Z",
     "start_time": "2024-11-21T08:18:14.341277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "9032fe5f50d1192c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Applying chat template",
   "id": "7a1cda242ab739f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:18:24.871583Z",
     "start_time": "2024-11-21T08:18:24.843677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_data = dataset[\"test\"]\n",
    "instruction =\"You are a helpful programmer instruction\"\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"user\", \"content\": \"write a function that takes a list of integers and returns the sum of all the integers in the list in java.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(text)"
   ],
   "id": "2de07b950bda9410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "write a function that takes a list of integers and returns the sum of all the integers in the list in java.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T06:38:15.913694Z",
     "start_time": "2024-11-21T06:38:15.907570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(text)"
   ],
   "id": "96d2d3804f05b630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    106,   1645,    108,   4086,    476,   1411,    674,   5548,\n",
      "            476,   1889,    576,  45047,    578,   8753,    573,   2707,    576,\n",
      "            832,    573,  45047,    575,    573,   1889,    575,   1821, 235265,\n",
      "            107,    108,    106,   2516,    108]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T06:38:42.530135Z",
     "start_time": "2024-11-21T06:38:26.649169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(input_ids=text, do_sample=True, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "674e8cc221bd2fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "write a function that takes a list of integers and returns the sum of all the integers in the list in java.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "```java\n",
      "import java.util.List;\n",
      "\n",
      "public class SumList {\n",
      "\n",
      "    public static int sum(List<Integer> numbers) {\n",
      "        int sum = 0;\n",
      "        for (Integer number : numbers) { \n",
      "            sum += number; \n",
      "        }\n",
      "        return sum;\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        List<Integer> numberList = List.of(1, 2, 3, 4, 5);\n",
      "        int totalSum = sum(numberList);\n",
      "\n",
      "        System.out.println(\"The sum of the numbers is: \" + totalSum);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Function Definition:**  \n",
      "    * `public static int sum(List<Integer> numbers)`: \n",
      "        * `public` means the method is accessible from other parts of your code.\n",
      "        * `static` means it's\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Without chat template",
   "id": "4032c123dd0d7ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T07:12:16.857412Z",
     "start_time": "2024-11-21T07:12:16.851193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = \"Instruction : write a function that takes a list of integers and returns the sum of all the integers in the list in java. \\n Response:\"\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_ids"
   ],
   "id": "7eb678022dce9e98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,  37854,    865,   5598,    476,   1411,    674,   5548,    476,\n",
       "           1889,    576,  45047,    578,   8753,    573,   2707,    576,    832,\n",
       "            573,  45047,    575,    573,   1889,    575,   1821, 235265, 235248,\n",
       "            108,  10567, 235292]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T07:12:30.685944Z",
     "start_time": "2024-11-21T07:12:18.024726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(input_ids=input_ids['input_ids'], do_sample=True, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "901a87f658b83632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Instruction : write a function that takes a list of integers and returns the sum of all the integers in the list in java. \n",
      " Response: \n",
      " \n",
      " ```java\n",
      "public class Main {\n",
      "  public static int sumOfList(int[] list) {\n",
      "    int totalSum = 0;\n",
      "    for (int i = 0; i < list.length; i++) {\n",
      "      totalSum += list[i];\n",
      "    }\n",
      "    return totalSum;\n",
      "  }\n",
      "  \n",
      "  public static void main(String[] args) {\n",
      "    int[] numbers = {1, 2, 3, 4, 5};\n",
      "    System.out.println(sumOfList(numbers));\n",
      "  }\n",
      "}\n",
      "```\n",
      "  \n",
      " **The function `sumOfList` does the following:**\n",
      "\n",
      "1.  **Takes an integer array (list) as input using `int[] list`.**\n",
      "2.  **Initializes a variable `totalSum` to 0.** This variable will store the cumulative sum. \n",
      "3.  **Loops through the\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference for LLM for software Model Completion",
   "id": "7626e761723ec7d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from datasets import load_dataset",
   "id": "7526b2b2afb80921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Here is the model we use : ",
   "id": "5cfed663815b805c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "checkpoint = \"D:\\\\LLM\\\\thesisPractical\\\\fine_tuned_models\\\\gemma\\\\results\\\\checkpoint-500\"",
   "id": "245023f3d3a6ba49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### The following functions formats the input data to be used in the model",
   "id": "374b1fdf85b95057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def format_chat_template_to_print(input):\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": f'You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model :\\n{input}'}\n",
    "    ] \n",
    "    return tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def format_chat_template(input):\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": f'You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model :\\n{input}'}\n",
    "    ] \n",
    "    return tokenizer.apply_chat_template(row_json, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")"
   ],
   "id": "65ab0a99a40fa829"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load the model and tokenizer",
   "id": "d313ce60d8bf5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "188046dd5b916e7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load the dataset to test the model",
   "id": "431086da7ff9bd0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "org_path = \"D:\\LLM\\\\thesisPractical\\\\datasets\\\\structural_removal_non_contiguous\\\\processed_4000\"\n",
    "\n",
    "test_dataset_url = org_path + \"\\\\test.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'test' : test_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "test_dataset = dataset['test']"
   ],
   "id": "6f4c0e00849a3371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Apply the chat template to the input data",
   "id": "d6520f90634b16de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = test_dataset[0]['input']\n",
    "text_to_print = format_chat_template_to_print(data)\n",
    "print(text_to_print)\n",
    "input_ids = format_chat_template(data)"
   ],
   "id": "29a90bf61f1edcba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output = test_dataset[0]['output']\n",
    "print(output)"
   ],
   "id": "bfb5f0ed39bb1ee7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate the output from the model",
   "id": "e36864a821b7dbb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outputs = model.generate(**input_ids, max_length=3500)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ],
   "id": "a8fa58b6ebf033de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
