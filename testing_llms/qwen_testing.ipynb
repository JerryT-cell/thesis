{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This script is there to go through the Qwen model and see how it works.",
   "id": "156dd67fecff98c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start by importing the necessary libraries.",
   "id": "d23c3afecbc43e8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:03:47.381796Z",
     "start_time": "2024-11-21T08:03:45.320220Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, Starcoder2ForCausalLM, Qwen2ForCausalLM",
   "id": "4fe09365c9cf4c29",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "get the model and tokenizer from the checkpoint.",
   "id": "fc8009f7316074d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:03:54.650523Z",
     "start_time": "2024-11-21T08:03:54.647784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "device = \"cuda\""
   ],
   "id": "1bae35c7d52292dc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:03:55.999979Z",
     "start_time": "2024-11-21T08:03:55.449632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ],
   "id": "31e755d7f9af815e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:04:17.786614Z",
     "start_time": "2024-11-21T08:04:17.783960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_data = dataset[\"test\"]\n",
    "instruction =\"You are a helpful programmer instruction\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"write a function that takes a list of integers and returns the sum of all the integers in the list in java.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"write a function that takes a list of integers and returns the sum of all the integers in the list in java.\"}]"
   ],
   "id": "2fcdddc96b0b38a2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the model with a simple input.",
   "id": "15094c93541ff3ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:04:20.245608Z",
     "start_time": "2024-11-21T08:04:20.242971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ],
   "id": "386aa4c5ef708e8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful programmer instruction<|im_end|>\n",
      "<|im_start|>user\n",
      "write a function that takes a list of integers and returns the sum of all the integers in the list in java.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "write a function that takes a list of integers and returns the sum of all the integers in the list in java.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T20:31:11.221190Z",
     "start_time": "2024-11-19T20:30:19.849355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_ids0 = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=300\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids0)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ],
   "id": "742f916c9c8e3fdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is a simple Java function that takes a list of integers and returns the sum of all the integers in the list:\n",
      "\n",
      "```java\n",
      "import java.util.ArrayList;\n",
      "import java.util.Arrays;\n",
      "\n",
      "public class SumCalculator {\n",
      "\n",
      "    /**\n",
      "     * Calculates the sum of all elements in the given list of integers.\n",
      "     *\n",
      "     * @param numbers An ArrayList of Integer values.\n",
      "     * @return The sum of all elements in the list.\n",
      "     */\n",
      "    public static int calculateSum(ArrayList<Integer> numbers) {\n",
      "        return numbers.stream().mapToInt(Integer::intValue).sum();\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        // Example usage:\n",
      "        ArrayList<Integer> exampleList = new ArrayList<>(Arrays.asList(1, 2, 3, 4, 5));\n",
      "        System.out.println(\"The sum of the list is: \" + calculateSum(exampleList));\n",
      "        \n",
      "        // Additional test cases\n",
      "        ArrayList<Integer> emptyList = new ArrayList<>();\n",
      "        System.out.println(\"The sum of the empty list is: \" + calculateSum(emptyList));\n",
      "        \n",
      "        ArrayList<Integer> mixedList = new ArrayList<>(Arrays.asList(-1, 0, 1, -2, 3));\n",
      "        System.out.println(\"The sum of the mixed list is: \" + calculateSum(mixedList));\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **Import Statements**: We import `ArrayList` from `java.util` to use it and `Arrays.asList` to create an example list.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T20:29:04.365376Z",
     "start_time": "2024-11-19T20:29:04.361463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print(generated_ids)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ],
   "id": "c0e0816358517e3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is a simple Java function that takes a list of integers and returns the sum of all the integers in the list:\n",
      "\n",
      "```java\n",
      "import java.util.ArrayList;\n",
      "import java.util.Arrays;\n",
      "\n",
      "public class SumCalculator {\n",
      "    /**\n",
      "     * Calculates the sum of all elements in the given list.\n",
      "     * \n",
      "     * @param numbers An ArrayList of Integers.\n",
      "     * @return The sum of all elements in the list.\n",
      "     */\n",
      "    public static int calculateSum(ArrayList<Integer> numbers) {\n",
      "        return numbers.stream().mapToInt(Integer::intValue).sum();\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        // Example usage:\n",
      "        ArrayList<Integer> exampleList = new ArrayList<>(Arrays.asList(1, 2, 3, 4, 5));\n",
      "        System.out.println(\"The sum of the list is: \" + calculateSum(exampleList));\n",
      "        \n",
      "        // Additional test cases\n",
      "        ArrayList<Integer> emptyList = new ArrayList<>();\n",
      "        System.out.println(\"The sum of an empty list is: \" + calculateSum(emptyList));\n",
      "        \n",
      "        ArrayList<Integer> mixedList = new ArrayList<>(Arrays.asList(-1, -2, 3, 4, -5));\n",
      "        System.out.println(\"The sum of the mixed list is: \" + calculateSum(mixedList));\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **`ArrayList<Integer>`**: This is used to store the integers.\n",
      "- **`calculateSum` method**: This method uses Java Streams to convert\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LLMs for software Model Completion",
   "id": "39c2657c21e8c73f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T10:56:16.107631Z",
     "start_time": "2024-11-24T10:56:14.051107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "id": "32dcb98d575125bd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T10:56:16.113716Z",
     "start_time": "2024-11-24T10:56:16.109069Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = \"D:\\\\LLM\\\\thesisPractical\\\\fine_tuned_models\\\\qwen\\\\qwen3bCodeInstruct_finetunedModel\"",
   "id": "f43c7bd2801eb2f4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T10:56:16.213672Z",
     "start_time": "2024-11-24T10:56:16.199365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instruction = \"You are an AI assistant that specializes in UML model completion. Given an incomplete UML model represented in JSON format, output the missing portions of the model in JSON format.\"\n",
    "\n",
    "def format_chat_template_to_print(input):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the incomplete UML model:\\n{input}'}\n",
    "    ] \n",
    "    return tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def format_chat_template(input):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the incomplete UML model:\\n{input}'}\n",
    "    ] \n",
    "    return tokenizer.apply_chat_template(row_json, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")"
   ],
   "id": "92c14135e62af1a6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T10:58:08.660903Z",
     "start_time": "2024-11-24T10:56:36.751627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "4f9ea55ce0200438",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Some weights of the model checkpoint at D:\\LLM\\thesisPractical\\fine_tuned_models\\qwen\\qwen3bCodeInstruct_finetunedModel were not used when initializing Qwen2ForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.base_layer.weight.absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.base_layer.weight.absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.bias', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.bias', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.bias', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.base_layer.weight.absmax', 'model.layers.1.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.1.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.1.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.1.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.1.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.1.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.base_layer.weight.absmax', 'model.layers.1.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.1.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.1.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.bias', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.bias', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.bias', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.base_layer.weight.absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.base_layer.weight.absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.bias', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.bias', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.bias', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.base_layer.weight.absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.base_layer.weight.absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.bias', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.bias', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.bias', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.base_layer.weight.absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.base_layer.weight.absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.bias', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.bias', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.bias', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.base_layer.weight.absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.base_layer.weight.absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.bias', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.bias', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.bias', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.base_layer.weight.absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.base_layer.weight.absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.bias', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.bias', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.bias', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.base_layer.weight.absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.base_layer.weight.absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.bias', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.bias', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.bias', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.base_layer.weight.absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.base_layer.weight.absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.bias', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.bias', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.bias', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.base_layer.weight.absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.base_layer.weight.absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.bias', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.bias', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.bias', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.base_layer.weight', 'model.layers.18.mlp.down_proj.base_layer.weight.absmax', 'model.layers.18.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.18.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.18.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.18.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.18.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.18.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.18.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.18.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.gate_proj.lora_A.default.weight', 'model.layers.18.mlp.gate_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.base_layer.weight', 'model.layers.18.mlp.up_proj.base_layer.weight.absmax', 'model.layers.18.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.18.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.18.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.18.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.bias', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.bias', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.bias', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.base_layer.weight', 'model.layers.19.mlp.down_proj.base_layer.weight.absmax', 'model.layers.19.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.19.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.19.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.19.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.19.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.19.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.19.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.19.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.gate_proj.lora_A.default.weight', 'model.layers.19.mlp.gate_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.base_layer.weight', 'model.layers.19.mlp.up_proj.base_layer.weight.absmax', 'model.layers.19.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.19.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.19.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.19.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.bias', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.bias', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.bias', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.base_layer.weight.absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.base_layer.weight.absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.bias', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.bias', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.bias', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.base_layer.weight', 'model.layers.20.mlp.down_proj.base_layer.weight.absmax', 'model.layers.20.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.20.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.20.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.20.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.20.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.20.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.20.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.20.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.gate_proj.lora_A.default.weight', 'model.layers.20.mlp.gate_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.base_layer.weight', 'model.layers.20.mlp.up_proj.base_layer.weight.absmax', 'model.layers.20.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.20.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.20.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.20.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.bias', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.bias', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.bias', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.base_layer.weight', 'model.layers.21.mlp.down_proj.base_layer.weight.absmax', 'model.layers.21.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.21.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.21.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.21.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.21.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.21.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.21.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.21.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.gate_proj.lora_A.default.weight', 'model.layers.21.mlp.gate_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.base_layer.weight', 'model.layers.21.mlp.up_proj.base_layer.weight.absmax', 'model.layers.21.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.21.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.21.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.21.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.base_layer.bias', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.bias', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.bias', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.base_layer.weight', 'model.layers.22.mlp.down_proj.base_layer.weight.absmax', 'model.layers.22.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.22.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.22.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.22.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.22.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.22.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.22.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.22.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.gate_proj.lora_A.default.weight', 'model.layers.22.mlp.gate_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.base_layer.weight', 'model.layers.22.mlp.up_proj.base_layer.weight.absmax', 'model.layers.22.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.22.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.22.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.22.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.bias', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.bias', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.bias', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.base_layer.weight', 'model.layers.23.mlp.down_proj.base_layer.weight.absmax', 'model.layers.23.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.23.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.23.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.23.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.23.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.23.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.23.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.23.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.gate_proj.lora_A.default.weight', 'model.layers.23.mlp.gate_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.base_layer.weight', 'model.layers.23.mlp.up_proj.base_layer.weight.absmax', 'model.layers.23.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.23.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.23.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.23.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.bias', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.bias', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.bias', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.mlp.down_proj.base_layer.weight', 'model.layers.24.mlp.down_proj.base_layer.weight.absmax', 'model.layers.24.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.24.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.24.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.24.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.down_proj.lora_A.default.weight', 'model.layers.24.mlp.down_proj.lora_B.default.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.24.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.24.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.24.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.24.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.gate_proj.lora_A.default.weight', 'model.layers.24.mlp.gate_proj.lora_B.default.weight', 'model.layers.24.mlp.up_proj.base_layer.weight', 'model.layers.24.mlp.up_proj.base_layer.weight.absmax', 'model.layers.24.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.24.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.24.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.24.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.up_proj.lora_A.default.weight', 'model.layers.24.mlp.up_proj.lora_B.default.weight', 'model.layers.24.self_attn.k_proj.base_layer.bias', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.bias', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.bias', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.mlp.down_proj.base_layer.weight', 'model.layers.25.mlp.down_proj.base_layer.weight.absmax', 'model.layers.25.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.25.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.25.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.25.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.down_proj.lora_A.default.weight', 'model.layers.25.mlp.down_proj.lora_B.default.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.25.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.25.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.25.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.25.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.gate_proj.lora_A.default.weight', 'model.layers.25.mlp.gate_proj.lora_B.default.weight', 'model.layers.25.mlp.up_proj.base_layer.weight', 'model.layers.25.mlp.up_proj.base_layer.weight.absmax', 'model.layers.25.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.25.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.25.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.25.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.up_proj.lora_A.default.weight', 'model.layers.25.mlp.up_proj.lora_B.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.bias', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.bias', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.bias', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.mlp.down_proj.base_layer.weight', 'model.layers.26.mlp.down_proj.base_layer.weight.absmax', 'model.layers.26.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.26.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.26.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.26.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.down_proj.lora_A.default.weight', 'model.layers.26.mlp.down_proj.lora_B.default.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.26.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.26.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.26.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.26.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.gate_proj.lora_A.default.weight', 'model.layers.26.mlp.gate_proj.lora_B.default.weight', 'model.layers.26.mlp.up_proj.base_layer.weight', 'model.layers.26.mlp.up_proj.base_layer.weight.absmax', 'model.layers.26.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.26.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.26.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.26.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.up_proj.lora_A.default.weight', 'model.layers.26.mlp.up_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.base_layer.bias', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.bias', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.bias', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.mlp.down_proj.base_layer.weight', 'model.layers.27.mlp.down_proj.base_layer.weight.absmax', 'model.layers.27.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.27.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.27.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.27.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.down_proj.lora_A.default.weight', 'model.layers.27.mlp.down_proj.lora_B.default.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.27.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.27.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.27.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.27.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.gate_proj.lora_A.default.weight', 'model.layers.27.mlp.gate_proj.lora_B.default.weight', 'model.layers.27.mlp.up_proj.base_layer.weight', 'model.layers.27.mlp.up_proj.base_layer.weight.absmax', 'model.layers.27.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.27.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.27.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.27.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.up_proj.lora_A.default.weight', 'model.layers.27.mlp.up_proj.lora_B.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.bias', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.bias', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.bias', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.mlp.down_proj.base_layer.weight', 'model.layers.28.mlp.down_proj.base_layer.weight.absmax', 'model.layers.28.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.28.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.28.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.28.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.mlp.down_proj.lora_A.default.weight', 'model.layers.28.mlp.down_proj.lora_B.default.weight', 'model.layers.28.mlp.gate_proj.base_layer.weight', 'model.layers.28.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.28.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.28.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.28.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.28.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.mlp.gate_proj.lora_A.default.weight', 'model.layers.28.mlp.gate_proj.lora_B.default.weight', 'model.layers.28.mlp.up_proj.base_layer.weight', 'model.layers.28.mlp.up_proj.base_layer.weight.absmax', 'model.layers.28.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.28.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.28.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.28.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.mlp.up_proj.lora_A.default.weight', 'model.layers.28.mlp.up_proj.lora_B.default.weight', 'model.layers.28.self_attn.k_proj.base_layer.bias', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.28.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.28.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.28.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.28.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.bias', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.28.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.28.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.bias', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.28.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.28.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.mlp.down_proj.base_layer.weight', 'model.layers.29.mlp.down_proj.base_layer.weight.absmax', 'model.layers.29.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.29.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.29.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.29.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.mlp.down_proj.lora_A.default.weight', 'model.layers.29.mlp.down_proj.lora_B.default.weight', 'model.layers.29.mlp.gate_proj.base_layer.weight', 'model.layers.29.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.29.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.29.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.29.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.29.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.mlp.gate_proj.lora_A.default.weight', 'model.layers.29.mlp.gate_proj.lora_B.default.weight', 'model.layers.29.mlp.up_proj.base_layer.weight', 'model.layers.29.mlp.up_proj.base_layer.weight.absmax', 'model.layers.29.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.29.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.29.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.29.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.mlp.up_proj.lora_A.default.weight', 'model.layers.29.mlp.up_proj.lora_B.default.weight', 'model.layers.29.self_attn.k_proj.base_layer.bias', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.29.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.29.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.29.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.29.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.bias', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.29.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.29.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.bias', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.29.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.29.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.base_layer.weight.absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.base_layer.weight.absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.bias', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.bias', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.bias', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.mlp.down_proj.base_layer.weight', 'model.layers.30.mlp.down_proj.base_layer.weight.absmax', 'model.layers.30.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.30.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.30.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.30.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.mlp.down_proj.lora_A.default.weight', 'model.layers.30.mlp.down_proj.lora_B.default.weight', 'model.layers.30.mlp.gate_proj.base_layer.weight', 'model.layers.30.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.30.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.30.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.30.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.30.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.mlp.gate_proj.lora_A.default.weight', 'model.layers.30.mlp.gate_proj.lora_B.default.weight', 'model.layers.30.mlp.up_proj.base_layer.weight', 'model.layers.30.mlp.up_proj.base_layer.weight.absmax', 'model.layers.30.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.30.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.30.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.30.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.mlp.up_proj.lora_A.default.weight', 'model.layers.30.mlp.up_proj.lora_B.default.weight', 'model.layers.30.self_attn.k_proj.base_layer.bias', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.30.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.30.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.30.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.30.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.bias', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.30.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.30.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.bias', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.30.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.30.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.mlp.down_proj.base_layer.weight', 'model.layers.31.mlp.down_proj.base_layer.weight.absmax', 'model.layers.31.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.31.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.31.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.31.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.mlp.down_proj.lora_A.default.weight', 'model.layers.31.mlp.down_proj.lora_B.default.weight', 'model.layers.31.mlp.gate_proj.base_layer.weight', 'model.layers.31.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.31.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.31.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.31.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.31.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.mlp.gate_proj.lora_A.default.weight', 'model.layers.31.mlp.gate_proj.lora_B.default.weight', 'model.layers.31.mlp.up_proj.base_layer.weight', 'model.layers.31.mlp.up_proj.base_layer.weight.absmax', 'model.layers.31.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.31.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.31.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.31.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.mlp.up_proj.lora_A.default.weight', 'model.layers.31.mlp.up_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.bias', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.31.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.31.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.31.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.31.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.bias', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.31.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.31.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.bias', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.31.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.31.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.mlp.down_proj.base_layer.weight', 'model.layers.32.mlp.down_proj.base_layer.weight.absmax', 'model.layers.32.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.32.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.32.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.32.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.mlp.down_proj.lora_A.default.weight', 'model.layers.32.mlp.down_proj.lora_B.default.weight', 'model.layers.32.mlp.gate_proj.base_layer.weight', 'model.layers.32.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.32.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.32.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.32.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.32.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.mlp.gate_proj.lora_A.default.weight', 'model.layers.32.mlp.gate_proj.lora_B.default.weight', 'model.layers.32.mlp.up_proj.base_layer.weight', 'model.layers.32.mlp.up_proj.base_layer.weight.absmax', 'model.layers.32.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.32.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.32.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.32.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.mlp.up_proj.lora_A.default.weight', 'model.layers.32.mlp.up_proj.lora_B.default.weight', 'model.layers.32.self_attn.k_proj.base_layer.bias', 'model.layers.32.self_attn.k_proj.base_layer.weight', 'model.layers.32.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.32.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.32.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.32.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.32.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.self_attn.k_proj.lora_A.default.weight', 'model.layers.32.self_attn.k_proj.lora_B.default.weight', 'model.layers.32.self_attn.o_proj.base_layer.weight', 'model.layers.32.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.32.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.32.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.32.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.32.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.self_attn.o_proj.lora_A.default.weight', 'model.layers.32.self_attn.o_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.bias', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.32.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.32.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.32.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.32.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.base_layer.bias', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.32.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.32.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.32.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.32.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.mlp.down_proj.base_layer.weight', 'model.layers.33.mlp.down_proj.base_layer.weight.absmax', 'model.layers.33.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.33.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.33.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.33.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.mlp.down_proj.lora_A.default.weight', 'model.layers.33.mlp.down_proj.lora_B.default.weight', 'model.layers.33.mlp.gate_proj.base_layer.weight', 'model.layers.33.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.33.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.33.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.33.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.33.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.mlp.gate_proj.lora_A.default.weight', 'model.layers.33.mlp.gate_proj.lora_B.default.weight', 'model.layers.33.mlp.up_proj.base_layer.weight', 'model.layers.33.mlp.up_proj.base_layer.weight.absmax', 'model.layers.33.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.33.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.33.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.33.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.mlp.up_proj.lora_A.default.weight', 'model.layers.33.mlp.up_proj.lora_B.default.weight', 'model.layers.33.self_attn.k_proj.base_layer.bias', 'model.layers.33.self_attn.k_proj.base_layer.weight', 'model.layers.33.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.33.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.33.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.33.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.33.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.self_attn.k_proj.lora_A.default.weight', 'model.layers.33.self_attn.k_proj.lora_B.default.weight', 'model.layers.33.self_attn.o_proj.base_layer.weight', 'model.layers.33.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.33.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.33.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.33.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.33.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.self_attn.o_proj.lora_A.default.weight', 'model.layers.33.self_attn.o_proj.lora_B.default.weight', 'model.layers.33.self_attn.q_proj.base_layer.bias', 'model.layers.33.self_attn.q_proj.base_layer.weight', 'model.layers.33.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.33.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.33.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.33.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.33.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.self_attn.q_proj.lora_A.default.weight', 'model.layers.33.self_attn.q_proj.lora_B.default.weight', 'model.layers.33.self_attn.v_proj.base_layer.bias', 'model.layers.33.self_attn.v_proj.base_layer.weight', 'model.layers.33.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.33.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.33.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.33.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.33.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.33.self_attn.v_proj.lora_A.default.weight', 'model.layers.33.self_attn.v_proj.lora_B.default.weight', 'model.layers.34.mlp.down_proj.base_layer.weight', 'model.layers.34.mlp.down_proj.base_layer.weight.absmax', 'model.layers.34.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.34.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.34.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.34.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.mlp.down_proj.lora_A.default.weight', 'model.layers.34.mlp.down_proj.lora_B.default.weight', 'model.layers.34.mlp.gate_proj.base_layer.weight', 'model.layers.34.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.34.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.34.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.34.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.34.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.mlp.gate_proj.lora_A.default.weight', 'model.layers.34.mlp.gate_proj.lora_B.default.weight', 'model.layers.34.mlp.up_proj.base_layer.weight', 'model.layers.34.mlp.up_proj.base_layer.weight.absmax', 'model.layers.34.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.34.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.34.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.34.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.mlp.up_proj.lora_A.default.weight', 'model.layers.34.mlp.up_proj.lora_B.default.weight', 'model.layers.34.self_attn.k_proj.base_layer.bias', 'model.layers.34.self_attn.k_proj.base_layer.weight', 'model.layers.34.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.34.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.34.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.34.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.34.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.self_attn.k_proj.lora_A.default.weight', 'model.layers.34.self_attn.k_proj.lora_B.default.weight', 'model.layers.34.self_attn.o_proj.base_layer.weight', 'model.layers.34.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.34.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.34.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.34.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.34.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.self_attn.o_proj.lora_A.default.weight', 'model.layers.34.self_attn.o_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.bias', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.34.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.34.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.34.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.34.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.base_layer.bias', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.34.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.34.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.34.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.34.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.mlp.down_proj.base_layer.weight', 'model.layers.35.mlp.down_proj.base_layer.weight.absmax', 'model.layers.35.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.35.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.35.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.35.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.mlp.down_proj.lora_A.default.weight', 'model.layers.35.mlp.down_proj.lora_B.default.weight', 'model.layers.35.mlp.gate_proj.base_layer.weight', 'model.layers.35.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.35.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.35.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.35.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.35.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.mlp.gate_proj.lora_A.default.weight', 'model.layers.35.mlp.gate_proj.lora_B.default.weight', 'model.layers.35.mlp.up_proj.base_layer.weight', 'model.layers.35.mlp.up_proj.base_layer.weight.absmax', 'model.layers.35.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.35.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.35.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.35.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.mlp.up_proj.lora_A.default.weight', 'model.layers.35.mlp.up_proj.lora_B.default.weight', 'model.layers.35.self_attn.k_proj.base_layer.bias', 'model.layers.35.self_attn.k_proj.base_layer.weight', 'model.layers.35.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.35.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.35.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.35.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.35.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.self_attn.k_proj.lora_A.default.weight', 'model.layers.35.self_attn.k_proj.lora_B.default.weight', 'model.layers.35.self_attn.o_proj.base_layer.weight', 'model.layers.35.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.35.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.35.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.35.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.35.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.self_attn.o_proj.lora_A.default.weight', 'model.layers.35.self_attn.o_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.bias', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.35.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.35.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.35.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.35.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.base_layer.bias', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.35.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.35.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.35.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.35.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.base_layer.weight.absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.base_layer.weight.absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.bias', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.bias', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.bias', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.base_layer.weight.absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.base_layer.weight.absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.bias', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.bias', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.bias', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.base_layer.weight.absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.base_layer.weight.absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.bias', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.bias', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.bias', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.base_layer.weight.absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.base_layer.weight.absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.bias', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.bias', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.bias', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.base_layer.weight.absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.base_layer.weight.absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.bias', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.bias', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.bias', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.base_layer.weight.absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.base_layer.weight.absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.bias', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.bias', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.bias', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at D:\\LLM\\thesisPractical\\fine_tuned_models\\qwen\\qwen3bCodeInstruct_finetunedModel and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.34.mlp.down_proj.weight', 'model.layers.34.mlp.gate_proj.weight', 'model.layers.34.mlp.up_proj.weight', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.mlp.down_proj.weight', 'model.layers.35.mlp.gate_proj.weight', 'model.layers.35.mlp.up_proj.weight', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 757448 column 3",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 10\u001B[0m\n\u001B[0;32m      1\u001B[0m quantization_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(\n\u001B[0;32m      2\u001B[0m     load_in_4bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m      3\u001B[0m     bnb_4bit_quant_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnf4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      4\u001B[0m     bnb_4bit_compute_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m      5\u001B[0m     bnb_4bit_use_double_quant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m      8\u001B[0m     checkpoint\n\u001B[0;32m      9\u001B[0m )\n\u001B[1;32m---> 10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(checkpoint)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:896\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    892\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    893\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    894\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTokenizer class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokenizer_class_candidate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist or is not currently imported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    895\u001B[0m         )\n\u001B[1;32m--> 896\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    898\u001B[0m \u001B[38;5;66;03m# Otherwise we have to be creative.\u001B[39;00m\n\u001B[0;32m    899\u001B[0m \u001B[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, EncoderDecoderConfig):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2291\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2288\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2289\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2291\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_from_pretrained(\n\u001B[0;32m   2292\u001B[0m     resolved_vocab_files,\n\u001B[0;32m   2293\u001B[0m     pretrained_model_name_or_path,\n\u001B[0;32m   2294\u001B[0m     init_configuration,\n\u001B[0;32m   2295\u001B[0m     \u001B[38;5;241m*\u001B[39minit_inputs,\n\u001B[0;32m   2296\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2297\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2298\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m   2299\u001B[0m     _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[0;32m   2300\u001B[0m     _is_local\u001B[38;5;241m=\u001B[39mis_local,\n\u001B[0;32m   2301\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2302\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2303\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2525\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2523\u001B[0m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[0;32m   2524\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2525\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39minit_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs)\n\u001B[0;32m   2526\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m   2527\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m   2528\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load vocabulary from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2529\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2530\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\models\\qwen2\\tokenization_qwen2_fast.py:120\u001B[0m, in \u001B[0;36mQwen2TokenizerFast.__init__\u001B[1;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001B[0m\n\u001B[0;32m    109\u001B[0m unk_token \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    110\u001B[0m     AddedToken(unk_token, lstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, rstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, special\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, normalized\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(unk_token, \u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m unk_token\n\u001B[0;32m    113\u001B[0m )\n\u001B[0;32m    114\u001B[0m pad_token \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    115\u001B[0m     AddedToken(pad_token, lstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, rstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, special\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, normalized\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pad_token, \u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m pad_token\n\u001B[0;32m    118\u001B[0m )\n\u001B[1;32m--> 120\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    121\u001B[0m     vocab_file\u001B[38;5;241m=\u001B[39mvocab_file,\n\u001B[0;32m    122\u001B[0m     merges_file\u001B[38;5;241m=\u001B[39mmerges_file,\n\u001B[0;32m    123\u001B[0m     tokenizer_file\u001B[38;5;241m=\u001B[39mtokenizer_file,\n\u001B[0;32m    124\u001B[0m     unk_token\u001B[38;5;241m=\u001B[39munk_token,\n\u001B[0;32m    125\u001B[0m     bos_token\u001B[38;5;241m=\u001B[39mbos_token,\n\u001B[0;32m    126\u001B[0m     eos_token\u001B[38;5;241m=\u001B[39meos_token,\n\u001B[0;32m    127\u001B[0m     pad_token\u001B[38;5;241m=\u001B[39mpad_token,\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    129\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:115\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(tokenizer_object)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m fast_tokenizer_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m from_slow:\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001B[39;00m\n\u001B[1;32m--> 115\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m TokenizerFast\u001B[38;5;241m.\u001B[39mfrom_file(fast_tokenizer_file)\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m slow_tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001B[39;00m\n\u001B[0;32m    118\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001B[1;31mException\u001B[0m: data did not match any variant of untagged enum ModelWrapper at line 757448 column 3"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "org_path = \"D:\\LLM\\\\thesisPractical\\\\datasets\\\\structural_removal_non_contiguous\\\\processed_4000\"\n",
    "\n",
    "test_dataset_url = org_path + \"\\\\test.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'test': test_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "test_dataset = dataset['test']"
   ],
   "id": "e2505bcbe6369143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = test_dataset[0]['input']\n",
    "text_to_print = format_chat_template_to_print(data)\n",
    "print(text_to_print)\n",
    "input_ids = format_chat_template(data)"
   ],
   "id": "8b703743aa0ca8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output = test_dataset[0]['output']\n",
    "print(output)"
   ],
   "id": "a8a5b777850d18a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outputs = model.generate(**input_ids, max_length=3500)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ],
   "id": "8a0d2f1ab7c5e872"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
