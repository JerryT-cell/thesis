{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:44:46.128830Z",
     "start_time": "2024-11-21T04:44:46.125824Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### Running the model on a CPU",
   "id": "9fbd06774e2a9e43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "checkpoint = \"google/gemma-2b\"",
   "id": "c6ccac3df5a073a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ],
   "id": "5ceb5074664a7751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_text = \"Write a function to sort a list \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)"
   ],
   "id": "8493e8bac3e77137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "60eb28789fc4cc42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### Running the model on a GPU",
   "id": "3655856ce3d41b13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:38:06.269962Z",
     "start_time": "2024-11-21T04:38:06.267720Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = \"google/gemma-2b\"",
   "id": "49a2d12eb53d2401",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:38:12.050581Z",
     "start_time": "2024-11-21T04:38:07.393638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")"
   ],
   "id": "43ec0c5394fa0e4a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7317eadba0943f19a6f4e3ba4b05269"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:42:47.820929Z",
     "start_time": "2024-11-21T04:42:47.816231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"def print_hello_world(): \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")"
   ],
   "id": "ee73b96d2c1a7259",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:42:52.365772Z",
     "start_time": "2024-11-21T04:42:48.728436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(**input_ids, max_new_tokens=200, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "fc033cd3468b6282",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>def print_hello_world(): \n",
      "   print(\"Hello, World!\") \n",
      "\n",
      "def get_name(): \n",
      "   name = input(\"What is your name? \") \n",
      "   return name \n",
      "\n",
      "\n",
      "if __name__==\"__main__\": \n",
      "   print_hello_world() \n",
      "   my_name = get_name() \n",
      "   print(f\"Hello {my_name}\")\n",
      "<eos>\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running on GPU Using 4-bit precision and GPU",
   "id": "b809fae12f06f0cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:44:50.309579Z",
     "start_time": "2024-11-21T04:44:50.306702Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = \"google/gemma-2b\"                # change the model name to the one you want to use",
   "id": "129596f9182f2b2d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:45:24.911955Z",
     "start_time": "2024-11-21T04:45:21.946605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "fd9cd4ea6a84d58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd3c5287bf1649adabf3d07b1144c924"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:52:24.711590Z",
     "start_time": "2024-11-21T04:52:24.703841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"write a function that takes a list of integers and returns the sum of the list. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")"
   ],
   "id": "e28ee4968bc085b6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T04:52:42.271699Z",
     "start_time": "2024-11-21T04:52:34.024098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(**input_ids, do_sample=True, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "d1a0329e1024402e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>write a function that takes a list of integers and returns the sum of the list. 1. no other list functions can be used in your function 2. your function signature must be sumList(list)\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/3\n",
      "1. First, we need to check if the list is empty. If it is, we can return 0. Otherwise, we can continue with the next step.\n",
      "\n",
      "Step 2/3\n",
      "2. We need to take the first element of the list and add it to the sum. This is because the list has only one element, so we need to sum it up.\n",
      "\n",
      "Step 3/3\n",
      "3. We need to repeat step 2 for all elements of the list. This is because we want to continue summing up elements as long as the list is not empty. Here is the code: def sumList(myList): if len(myList) == 0: return 0 # initialize the sum to 0 sum = 0 for num in myList: sum += num return sum\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference for LLM for software Model Completion",
   "id": "4a64f1f325c3ebbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from datasets import load_dataset",
   "id": "2a4f748d786250c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "checkpoint = \"D:\\\\LLM\\\\thesisPractical\\\\fine_tuned_models\\\\gemma\\\\results\\\\checkpoint-500\"",
   "id": "64b69d0591512613"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "bda74068315e2226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset from processed 4000",
   "id": "a8351500cc11e8ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "org_path = \"D:\\LLM\\\\thesisPractical\\\\datasets\\\\structural_removal_non_contiguous\\\\processed_4000\"\n",
    "\n",
    "test_dataset_url = org_path + \"\\\\test.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'test' : test_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "test_dataset = dataset['test']"
   ],
   "id": "76460d33f3e0a220"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = test_dataset[0]['input']\n",
    "output = test_dataset[0]['output']\n",
    "print(data)"
   ],
   "id": "11020daaf3383702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "input_ids = tokenizer(data, padding=True,return_tensors='pt', truncation=True, max_length=3500).to(\"cuda\")",
   "id": "efd763e0e37e5a28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outputs = model.generate(**input_ids, max_length=3500)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ],
   "id": "6e97b0064c8aea50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
