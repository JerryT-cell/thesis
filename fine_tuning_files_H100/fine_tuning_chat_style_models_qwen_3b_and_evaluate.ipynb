{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9fbce4",
   "metadata": {},
   "source": [
    "# Train the Model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eac753e205fde1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    Arguments for the fine_tuning\n",
    "    \"\"\"\n",
    "    base_model = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    fine_tuned_model = \"Qwen2.5-3B-Instruct-software-model_completion_fine_tuned\"\n",
    "    merged_model = \"Qwen/Qwen2.5-3B-Instruct-software-model_completion\"\n",
    "    dataset_name = \"/home/ubuntu/dataset/one_elem_processed_4000\"\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=4)\n",
    "    evaluation_strategy: Optional[str] = field(default=\"steps\")\n",
    "    evaluation_accumulation_steps: Optional[int] = field(default=5)\n",
    "    learning_rate: Optional[float] = field(default=2e-4)\n",
    "    max_grad_norm: Optional[float] = field(default=0.3)\n",
    "    weight_decay: Optional[int] = field(default=0.001)\n",
    "    lora_alpha= 64,\n",
    "    lora_dropout =  0.5,\n",
    "    lora_r = 32\n",
    "    max_seq_length: Optional[int] = field(default=4100)\n",
    "    fp16 = True\n",
    "    bf16 = False\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    use_flash_attention_2: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables Flash Attention 2.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
    "    )\n",
    "    max_steps: int = field(default=100, metadata={\"help\": \"How many optimizer update steps to take\"}),\n",
    "    epochs : int = field(default=1, metadata={\"help\": \"How many epochs to train for\"})\n",
    "    warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n",
    "    save_steps: int = field(default=87, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    logging_steps: int = field(default=87, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    output_dir: str = field(\n",
    "        default=\"./qwen3b_instruct/results\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    logging_dir: str = field(\n",
    "        default=\"./qwen3b_instruct/logs\",\n",
    "        metadata={\"help\": \"The output directory where the logs will be written.\"},\n",
    "    )\n",
    "    eval_steps: int = field(default=87, metadata={\"help\": \"How often to evaluate the model\"})\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "# Parse the arguments, ignoring unrecognized ones\n",
    "script_args, remaining_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c0c7147713166cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df9044a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb5ce2a848cebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2f3c115bb144a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2978119f94a64eba8334b4022b1a7f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscript_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquantization_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_implementation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meager\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Load tokenizer\u001B[39;00m\n\u001B[1;32m     10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(script_args\u001B[38;5;241m.\u001B[39mbase_model)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4303\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   4300\u001B[0m         device_map_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffload_buffers\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   4302\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_deepspeed_zero3_enabled():\n\u001B[0;32m-> 4303\u001B[0m         \u001B[43mdispatch_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdevice_map_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   4306\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mpostprocess_model(model)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/big_modeling.py:420\u001B[0m, in \u001B[0;36mdispatch_model\u001B[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001B[0m\n\u001B[1;32m    415\u001B[0m         tied_params_map[data_ptr] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    417\u001B[0m         \u001B[38;5;66;03m# Note: To handle the disk offloading case, we can not simply use weights_map[param_name].data_ptr() as the reference pointer,\u001B[39;00m\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;66;03m# as we have no guarantee that safetensors' `file.get_tensor()` will always give the same pointer.\u001B[39;00m\n\u001B[0;32m--> 420\u001B[0m \u001B[43mattach_align_device_hook_on_blocks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    424\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffload_buffers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_buffers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    425\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweights_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_module_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_module_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;66;03m# warn if there is any params on the meta device\u001B[39;00m\n\u001B[1;32m    432\u001B[0m offloaded_devices_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m    433\u001B[0m     [device \u001B[38;5;28;01mfor\u001B[39;00m device \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(device_map\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisk\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[1;32m    434\u001B[0m )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:617\u001B[0m, in \u001B[0;36mattach_align_device_hook_on_blocks\u001B[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001B[0m\n\u001B[1;32m    608\u001B[0m     hook \u001B[38;5;241m=\u001B[39m AlignDevicesHook(\n\u001B[1;32m    609\u001B[0m         execution_device\u001B[38;5;241m=\u001B[39mexecution_device[module_name],\n\u001B[1;32m    610\u001B[0m         offload_buffers\u001B[38;5;241m=\u001B[39moffload_buffers,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    614\u001B[0m         tied_params_map\u001B[38;5;241m=\u001B[39mtied_params_map,\n\u001B[1;32m    615\u001B[0m     )\n\u001B[1;32m    616\u001B[0m     add_hook_to_module(module, hook)\n\u001B[0;32m--> 617\u001B[0m     \u001B[43mattach_execution_device_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m module_name \u001B[38;5;129;01min\u001B[39;00m execution_device \u001B[38;5;129;01mand\u001B[39;00m module_name \u001B[38;5;129;01min\u001B[39;00m offload:\n\u001B[1;32m    621\u001B[0m     attach_align_device_hook(\n\u001B[1;32m    622\u001B[0m         module,\n\u001B[1;32m    623\u001B[0m         execution_device\u001B[38;5;241m=\u001B[39mexecution_device[module_name],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    630\u001B[0m         tied_params_map\u001B[38;5;241m=\u001B[39mtied_params_map,\n\u001B[1;32m    631\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:439\u001B[0m, in \u001B[0;36mattach_execution_device_hook\u001B[0;34m(module, execution_device, skip_keys, preload_module_classes, tied_params_map)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 439\u001B[0m     \u001B[43mattach_execution_device_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:439\u001B[0m, in \u001B[0;36mattach_execution_device_hook\u001B[0;34m(module, execution_device, skip_keys, preload_module_classes, tied_params_map)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 439\u001B[0m     \u001B[43mattach_execution_device_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping similar frames: attach_execution_device_hook at line 439 (2 times)]\u001B[0m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:439\u001B[0m, in \u001B[0;36mattach_execution_device_hook\u001B[0;34m(module, execution_device, skip_keys, preload_module_classes, tied_params_map)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 439\u001B[0m     \u001B[43mattach_execution_device_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:429\u001B[0m, in \u001B[0;36mattach_execution_device_hook\u001B[0;34m(module, execution_device, skip_keys, preload_module_classes, tied_params_map)\u001B[0m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;124;03mRecursively attaches `AlignDevicesHook` to all submodules of a given model to make sure they have the right\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;124;03mexecution device\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;124;03m        instead of duplicating memory.\u001B[39;00m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_hf_hook\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(module\u001B[38;5;241m.\u001B[39mstate_dict()) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 429\u001B[0m     \u001B[43madd_hook_to_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mAlignDevicesHook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m# Break the recursion if we get to a preload module.\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m preload_module_classes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m preload_module_classes:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:161\u001B[0m, in \u001B[0;36madd_hook_to_module\u001B[0;34m(module, hook, append)\u001B[0m\n\u001B[1;32m    158\u001B[0m     old_forward \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39mforward\n\u001B[1;32m    159\u001B[0m     module\u001B[38;5;241m.\u001B[39m_old_forward \u001B[38;5;241m=\u001B[39m old_forward\n\u001B[0;32m--> 161\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[43mhook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m module\u001B[38;5;241m.\u001B[39m_hf_hook \u001B[38;5;241m=\u001B[39m hook\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnew_forward\u001B[39m(module, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:283\u001B[0m, in \u001B[0;36mAlignDevicesHook.init_hook\u001B[0;34m(self, module)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffload \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecution_device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, _ \u001B[38;5;129;01min\u001B[39;00m named_module_tensors(module, recurse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_submodules):\n\u001B[0;32m--> 283\u001B[0m         \u001B[43mset_module_tensor_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffload:\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moriginal_devices \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    286\u001B[0m         name: param\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;28;01mfor\u001B[39;00m name, param \u001B[38;5;129;01min\u001B[39;00m named_module_tensors(module, recurse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_submodules)\n\u001B[1;32m    287\u001B[0m     }\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:297\u001B[0m, in \u001B[0;36mset_module_tensor_to_device\u001B[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001B[0m\n\u001B[1;32m    294\u001B[0m         value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mto(dtype)\n\u001B[1;32m    296\u001B[0m device_quantization \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 297\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mno_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;66;03m# leave it on cpu first before moving them to cuda\u001B[39;00m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# # fix the case where the device is meta, we don't want to put it on cpu because there is no data =0\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    301\u001B[0m         param \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m param\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdevice(device)\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    304\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m param_cls\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInt8Params\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP4Params\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    305\u001B[0m     ):\n\u001B[1;32m    306\u001B[0m         device_quantization \u001B[38;5;241m=\u001B[39m device\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:154\u001B[0m, in \u001B[0;36m_NoParamDecoratorContextManager.__new__\u001B[0;34m(cls, orig_func)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_NoParamDecoratorContextManager\u001B[39;00m(_DecoratorContextManager):\n\u001B[1;32m    152\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Allow a context manager to be used as a decorator without parentheses.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, orig_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m orig_func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    156\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291f8c352beb151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'down_proj', 'k_proj', 'up_proj', 'q_proj', 'gate_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349660f34e7ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bdc45de2d2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for kbit training\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d35cfa26177251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "abs_path = script_args.dataset_name\n",
    "train_dataset_url = f\"{abs_path}/train.jsonl\"\n",
    "test_dataset_url = f\"{abs_path}/test.jsonl\"\n",
    "validation_dataset_url = f\"{abs_path}/validation.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'train': train_dataset_url,\n",
    "    'test': test_dataset_url,\n",
    "    'validation': validation_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815556e1e8cf1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "instruction = \"You are an AI assistant that specializes in UML model completion. Given an incomplete UML model represented in JSON format, output the missing portions of the model in JSON format.\"\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the incomplete UML model:\\n{row[\"input\"]}'},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"output\"]}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "trained_data = train_dataset.map(format_chat_template)\n",
    "validation_data = validation_dataset.map(format_chat_template)\n",
    "test_data = test_dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7595d611045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    save_steps=script_args.save_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    optim=script_args.optim,\n",
    "    num_train_epochs=script_args.epochs,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    eval_strategy=script_args.evaluation_strategy,\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    eval_accumulation_steps=script_args.evaluation_accumulation_steps,\n",
    "    logging_dir=script_args.logging_dir,\n",
    "    warmup_ratio=script_args.warmup_ratio,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    max_seq_length= script_args.max_seq_length,\n",
    "    fp16=script_args.fp16,\n",
    "    bf16=script_args.bf16,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880488374e29731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=trained_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=script_args.max_seq_length,\n",
    "    dataset_text_field=\"text\"\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2febe62973fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 42:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.082275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=145, training_loss=0.10631517870672817, metrics={'train_runtime': 2587.5888, 'train_samples_per_second': 0.224, 'train_steps_per_second': 0.056, 'total_flos': 2.6866241010180096e+16, 'train_loss': 0.10631517870672817, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac93fe",
   "metadata": {},
   "source": [
    "# Save the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea42084b603079",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(script_args.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367749ed4389a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bfce57871e4cf6af8a9d114ed3d6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83861593e2a54994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "#base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "model = PeftModel.from_pretrained(base_model_reload, script_args.fine_tuned_model)\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a788cc6f72d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen2.5-3B-Instruct-software-model_completion/tokenizer_config.json',\n",
       " 'Qwen/Qwen2.5-3B-Instruct-software-model_completion/special_tokens_map.json',\n",
       " 'Qwen/Qwen2.5-3B-Instruct-software-model_completion/vocab.json',\n",
       " 'Qwen/Qwen2.5-3B-Instruct-software-model_completion/merges.txt',\n",
       " 'Qwen/Qwen2.5-3B-Instruct-software-model_completion/added_tokens.json',\n",
       " 'Qwen/Qwen2.5-3B-Instruct-software-model_completion/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(script_args.merged_model)\n",
    "tokenizer.save_pretrained(script_args.merged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f41dc",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "688e24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d59dac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/home/ubuntu/fine-tuning/Qwen/Qwen2.5-3B-Instruct-software-model_completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab9a3bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3368cc5edaa459f86d07669a8acfef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint, device_map=\"auto\", quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "391c85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(row):\n",
    "    # Tokenize the \"output\" text\n",
    "    tokens_output = tokenizer(row['output'], truncation=False)  # En\n",
    "    tokens_input= tokenizer(row['input'], truncation=False) \n",
    "    \n",
    "    # Compute the length (number of tokens)\n",
    "    return {\n",
    "        'output_length': len(tokens_output['input_ids']),\n",
    "        'input_length': len(tokens_input['input_ids'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6326de94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bea026c134e44a68133cf0510bf4d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3709\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(compute_length)\n",
    "max_length = test_dataset['output_length']\n",
    "max_length_output = max(max_length)\n",
    "max_length = test_dataset['input_length']\n",
    "max_length_input = max(max_length)\n",
    "print(max_length_input)\n",
    "print(max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d91049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the incomplete UML model:\\n{row[\"input\"]}'}\n",
    "    ]\n",
    "    tokenized_input  = tokenizer.apply_chat_template(row_json, tokenize=True, padding=True, padding_side=\"right\", add_generation_prompt=True ,max_length=4100)\n",
    "    #print(len(tokenized_input))\n",
    "    tokenized_output = tokenizer(row['output'], truncation=False)\n",
    "    #print(len(tokenized_output['input_ids']))\n",
    "    \n",
    "    row['input_ids'] = tokenized_input\n",
    "    row['labels'] = tokenized_output['input_ids']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "764ea197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00d620a4a0142ceb80defa6bd12dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815\n",
      "3461\n",
      "3436\n",
      "3085\n",
      "1546\n",
      "3221\n",
      "3105\n",
      "2393\n",
      "2829\n",
      "3101\n",
      "1652\n",
      "3495\n",
      "2930\n",
      "1701\n",
      "2273\n",
      "2157\n",
      "1973\n",
      "2953\n",
      "2868\n",
      "1876\n",
      "3088\n",
      "2536\n",
      "1972\n",
      "2836\n",
      "2738\n",
      "3565\n",
      "2188\n",
      "2307\n",
      "3123\n",
      "1829\n",
      "3603\n",
      "3101\n",
      "3765\n",
      "3414\n",
      "3101\n",
      "1712\n",
      "1472\n",
      "1810\n",
      "1510\n",
      "3492\n",
      "1547\n",
      "3219\n",
      "3169\n",
      "1109\n",
      "2769\n",
      "1771\n",
      "2842\n",
      "2887\n",
      "3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2852: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the dataset\n",
    "test_dataset = test_dataset.map(format_chat_template)\n",
    "test_dataset = test_dataset.remove_columns([\"input\", \"output\",\"output_length\",\"input_length\"])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids','labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93fb0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encode)\n",
    "def align_predictions_labels(generated_ids, labels, pad_token_id, max_length):\n",
    "    \"\"\"\n",
    "    Aligns the lengths of predictions and labels by padding or truncating.\n",
    "\n",
    "    Args:\n",
    "        generated_ids (Tensor): Tensor of shape (batch_size, pred_seq_length)\n",
    "        labels (Tensor): Tensor of shape (batch_size, label_seq_length)\n",
    "        pad_token_id (int): The token ID used for padding.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Aligned predictions and labels.\n",
    "    \"\"\"\n",
    "    global true_predictions\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "    predicted_text = generated_text.split(\"assistant\\n\")[1]\n",
    "    predicted_tokens = tokenizer(predicted_text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)['input_ids']\n",
    "    \n",
    "    batch_size, pred_len = predicted_tokens.size()\n",
    "    _, label_len = labels.size()\n",
    "    print(\"prediction length: \")\n",
    "    print(pred_len)\n",
    "    print(\"label length: \")\n",
    "    print(label_len)\n",
    "\n",
    "    if pred_len < label_len:\n",
    "        # Pad predictions\n",
    "        padding = torch.full((batch_size, label_len - pred_len), pad_token_id, dtype=predicted_tokens.dtype).to( predicted_tokens.device)\n",
    "        true_predictions = torch.cat([predicted_tokens, padding], dim=1)\n",
    "    elif pred_len > label_len:\n",
    "        # Truncate predictions\n",
    "        true_predictions =  predicted_tokens[:, :label_len]\n",
    "    else:\n",
    "        true_predictions = predicted_tokens\n",
    "        \n",
    "    \n",
    "    batch_size, pred_len = true_predictions.size()\n",
    "    print(\"true prediction length: \")\n",
    "    print(pred_len)\n",
    "    if  pred_len!= label_len:\n",
    "        raise ValueError(\"The size of the predictions and labels should match.\")\n",
    "\n",
    "    return true_predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ed70e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb9f3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction length: \n",
      "64\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "66\n",
      "label length: \n",
      "47\n",
      "true prediction length: \n",
      "47\n",
      "prediction length: \n",
      "91\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "65\n",
      "label length: \n",
      "51\n",
      "true prediction length: \n",
      "51\n",
      "prediction length: \n",
      "89\n",
      "label length: \n",
      "103\n",
      "true prediction length: \n",
      "103\n",
      "prediction length: \n",
      "141\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "93\n",
      "label length: \n",
      "165\n",
      "true prediction length: \n",
      "165\n",
      "prediction length: \n",
      "68\n",
      "label length: \n",
      "70\n",
      "true prediction length: \n",
      "70\n",
      "prediction length: \n",
      "67\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "65\n",
      "label length: \n",
      "165\n",
      "true prediction length: \n",
      "165\n",
      "prediction length: \n",
      "99\n",
      "label length: \n",
      "98\n",
      "true prediction length: \n",
      "98\n",
      "prediction length: \n",
      "51\n",
      "label length: \n",
      "53\n",
      "true prediction length: \n",
      "53\n",
      "prediction length: \n",
      "68\n",
      "label length: \n",
      "183\n",
      "true prediction length: \n",
      "183\n",
      "prediction length: \n",
      "61\n",
      "label length: \n",
      "101\n",
      "true prediction length: \n",
      "101\n",
      "prediction length: \n",
      "106\n",
      "label length: \n",
      "131\n",
      "true prediction length: \n",
      "131\n",
      "prediction length: \n",
      "64\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "149\n",
      "label length: \n",
      "155\n",
      "true prediction length: \n",
      "155\n",
      "prediction length: \n",
      "95\n",
      "label length: \n",
      "93\n",
      "true prediction length: \n",
      "93\n",
      "prediction length: \n",
      "65\n",
      "label length: \n",
      "47\n",
      "true prediction length: \n",
      "47\n",
      "prediction length: \n",
      "77\n",
      "label length: \n",
      "85\n",
      "true prediction length: \n",
      "85\n",
      "prediction length: \n",
      "179\n",
      "label length: \n",
      "178\n",
      "true prediction length: \n",
      "178\n",
      "prediction length: \n",
      "150\n",
      "label length: \n",
      "44\n",
      "true prediction length: \n",
      "44\n",
      "prediction length: \n",
      "103\n",
      "label length: \n",
      "27\n",
      "true prediction length: \n",
      "27\n",
      "prediction length: \n",
      "63\n",
      "label length: \n",
      "51\n",
      "true prediction length: \n",
      "51\n",
      "prediction length: \n",
      "117\n",
      "label length: \n",
      "118\n",
      "true prediction length: \n",
      "118\n",
      "prediction length: \n",
      "66\n",
      "label length: \n",
      "76\n",
      "true prediction length: \n",
      "76\n",
      "prediction length: \n",
      "104\n",
      "label length: \n",
      "105\n",
      "true prediction length: \n",
      "105\n",
      "prediction length: \n",
      "119\n",
      "label length: \n",
      "73\n",
      "true prediction length: \n",
      "73\n",
      "prediction length: \n",
      "52\n",
      "label length: \n",
      "109\n",
      "true prediction length: \n",
      "109\n",
      "prediction length: \n",
      "62\n",
      "label length: \n",
      "60\n",
      "true prediction length: \n",
      "60\n",
      "prediction length: \n",
      "107\n",
      "label length: \n",
      "106\n",
      "true prediction length: \n",
      "106\n",
      "prediction length: \n",
      "65\n",
      "label length: \n",
      "165\n",
      "true prediction length: \n",
      "165\n",
      "prediction length: \n",
      "162\n",
      "label length: \n",
      "65\n",
      "true prediction length: \n",
      "65\n",
      "prediction length: \n",
      "62\n",
      "label length: \n",
      "102\n",
      "true prediction length: \n",
      "102\n",
      "prediction length: \n",
      "65\n",
      "label length: \n",
      "165\n",
      "true prediction length: \n",
      "165\n",
      "prediction length: \n",
      "61\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "99\n",
      "label length: \n",
      "98\n",
      "true prediction length: \n",
      "98\n",
      "prediction length: \n",
      "64\n",
      "label length: \n",
      "72\n",
      "true prediction length: \n",
      "72\n",
      "prediction length: \n",
      "62\n",
      "label length: \n",
      "157\n",
      "true prediction length: \n",
      "157\n",
      "prediction length: \n",
      "70\n",
      "label length: \n",
      "98\n",
      "true prediction length: \n",
      "98\n",
      "prediction length: \n",
      "48\n",
      "label length: \n",
      "105\n",
      "true prediction length: \n",
      "105\n",
      "prediction length: \n",
      "179\n",
      "label length: \n",
      "47\n",
      "true prediction length: \n",
      "47\n",
      "prediction length: \n",
      "117\n",
      "label length: \n",
      "126\n",
      "true prediction length: \n",
      "126\n",
      "prediction length: \n",
      "65\n",
      "label length: \n",
      "74\n",
      "true prediction length: \n",
      "74\n",
      "prediction length: \n",
      "79\n",
      "label length: \n",
      "96\n",
      "true prediction length: \n",
      "96\n",
      "prediction length: \n",
      "101\n",
      "label length: \n",
      "98\n",
      "true prediction length: \n",
      "98\n",
      "prediction length: \n",
      "63\n",
      "label length: \n",
      "45\n",
      "true prediction length: \n",
      "45\n",
      "prediction length: \n",
      "70\n",
      "label length: \n",
      "106\n",
      "true prediction length: \n",
      "106\n",
      "prediction length: \n",
      "114\n",
      "label length: \n",
      "101\n",
      "true prediction length: \n",
      "101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.2488849241748439}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "batch1 = next(iter(eval_dataloader))\n",
    "for batch in eval_dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    with torch.no_grad():\n",
    "         # Generate outputs\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_length_output,      # Ensure generation does not exceed allocated length\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False                        # Use deterministic generation; set to True for diversity\n",
    "        ) \n",
    "    aligned_preds, aligned_labels = align_predictions_labels(generated_ids, labels, tokenizer.pad_token_id, 4100)\n",
    "    \n",
    "    # Convert tensors to lists\n",
    "    preds = aligned_preds.cpu().tolist()\n",
    "    refs = aligned_labels.cpu().tolist()\n",
    "    \n",
    "    \n",
    "    # Flatten the lists for overall token-level accuracy\n",
    "    preds_flat = [token for seq in preds for token in seq]\n",
    "    refs_flat = [token for seq in refs for token in seq]\n",
    "    \n",
    "    metric.add_batch(predictions=preds_flat, references=refs_flat)\n",
    "\n",
    "metric.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b774f4f30109d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2bf916d50be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[\"test\"]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
