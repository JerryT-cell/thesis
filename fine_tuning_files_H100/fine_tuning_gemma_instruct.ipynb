{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:03.232171Z",
     "start_time": "2024-11-24T11:56:59.759880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac753e205fde1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:05.350715Z",
     "start_time": "2024-11-24T11:57:05.334624Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    Arguments for the fine_tuning\n",
    "    \"\"\"\n",
    "    base_model = \"google/gemma-2-2b-it\" \n",
    "    fine_tuned_model = \"gemma-2-2b-it-software-model_completion_finetuned\"\n",
    "    merged_model = \"gemma-2-2b-it-software-model_completion\"\n",
    "    dataset_name = \"/home/ubuntu/dataset/structural_removal_non_contiguous\"\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=4)\n",
    "    evaluation_strategy: Optional[str] = field(default=\"steps\")\n",
    "    evaluation_accumulation_steps: Optional[int] = field(default=5)\n",
    "    learning_rate: Optional[float] = field(default=2e-4)\n",
    "    max_grad_norm: Optional[float] = field(default=0.3)\n",
    "    weight_decay: Optional[int] = field(default=0.001)\n",
    "    lora_alpha= 64,\n",
    "    lora_dropout =  0.5,\n",
    "    lora_r = 32\n",
    "    max_seq_length: Optional[int] = field(default=4100)\n",
    "    fp16 = True\n",
    "    bf16 = False\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    use_flash_attention_2: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables Flash Attention 2.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
    "    )\n",
    "    max_steps: int = field(default=100, metadata={\"help\": \"How many optimizer update steps to take\"}),\n",
    "    epochs : int = field(default=3, metadata={\"help\": \"How many epochs to train for\"})\n",
    "    warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n",
    "    save_steps: int = field(default=87, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    logging_steps: int = field(default=87, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    output_dir: str = field(\n",
    "        default=\"./gemma2bit/results\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    logging_dir: str = field(\n",
    "        default=\"./gemma-2bit/logs\",\n",
    "        metadata={\"help\": \"The output directory where the logs will be written.\"},\n",
    "    )\n",
    "    eval_steps: int = field(default=87, metadata={\"help\": \"How often to evaluate the model\"})\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "# Parse the arguments, ignoring unrecognized ones\n",
    "script_args, remaining_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c7147713166cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:07.663525Z",
     "start_time": "2024-11-24T11:57:07.648409Z"
    }
   },
   "outputs": [],
   "source": [
    "access_token = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c88dbeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98b10f97b864839ae3d605f997f921b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5ce2a848cebb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:08.612363Z",
     "start_time": "2024-11-24T11:57:08.601847Z"
    }
   },
   "outputs": [],
   "source": [
    "# the quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2f3c115bb144a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:17.466177Z",
     "start_time": "2024-11-24T11:57:10.001516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf120c474a5e494db6f15a8c2f5d096f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ed7edfea814dcb93f201a68d2ef34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0085e7602c2349b2aaa82df092f6da7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71460c22905409eade9606346f84263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd8830215d34b83b264e1f69f75e4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d36d8419394feda0592c76da38b92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map =\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9cb8a40d43c9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8349660f34e7ce4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:17.472954Z",
     "start_time": "2024-11-24T11:57:17.466177Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3bdc45de2d2567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:17.842942Z",
     "start_time": "2024-11-24T11:57:17.499216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the model for kbit training\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d35cfa26177251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:19.173208Z",
     "start_time": "2024-11-24T11:57:18.534383Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "#windows\n",
    "# Load dataset\n",
    "'''org_path = \"D:\\LLM\\\\thesisPractical\\\\datasets_for_fine_tuning\\\\structural_removal_non_contiguous\\\\processed_2000\"\n",
    "\n",
    "train_dataset_url = org_path + \"\\\\train.jsonl\"\n",
    "test_dataset_url =org_path + \"\\\\test.jsonl\"\n",
    "validation_dataset_url =org_path + \"\\\\validation.jsonl\"'''\n",
    "\n",
    "#linux/abs\n",
    "\n",
    "abs_path = script_args.dataset_name\n",
    "dataset_to_use = \"processed_4000\"\n",
    "train_dataset_url = f\"{abs_path}/{dataset_to_use}/train.jsonl\"\n",
    "test_dataset_url = f\"{abs_path}/{dataset_to_use}/test.jsonl\"\n",
    "validation_dataset_url = f\"{abs_path}/{dataset_to_use}/validation.jsonl\"\n",
    "\n",
    "data_files = {\n",
    "    'train': train_dataset_url,\n",
    "    'test': test_dataset_url,\n",
    "    'validation': validation_dataset_url\n",
    "}\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815556e1e8cf1290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:21.408885Z",
     "start_time": "2024-11-24T11:57:20.697269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72be192544824fc3b8aa5398b75da8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20364c8f63f4cd39fed298c756cb3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96e2705a2b5446cb418997a6e513e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform the data\n",
    "instruction = \"\"\"You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model : \"\"\"\n",
    "\n",
    "def format_chat_template2(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": f'You are an AI assistant that specializes in UML model completion. Given the following incomplete UML model in Json format, complete the model by finding the missing part. Incomplete model :\\n{row[\"input\"]}'},\n",
    "        {\"role\": \"model\", \"content\": row[\"output\"]}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "trained_data = train_dataset.map(format_chat_template2)\n",
    "validation_data = validation_dataset.map(format_chat_template2)\n",
    "test_data = test_dataset.map(format_chat_template2)\n",
    "\n",
    "print(trained_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25b7595d611045fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:23.395091Z",
     "start_time": "2024-11-24T11:57:23.345316Z"
    }
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    save_steps=script_args.save_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    optim=script_args.optim,\n",
    "    num_train_epochs=script_args.epochs,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    eval_strategy=script_args.evaluation_strategy,\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    eval_accumulation_steps=script_args.evaluation_accumulation_steps,\n",
    "    logging_dir=script_args.logging_dir,\n",
    "    warmup_ratio=script_args.warmup_ratio,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    max_seq_length= script_args.max_seq_length,\n",
    "    fp16=script_args.fp16,\n",
    "    bf16=script_args.bf16,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "880488374e29731f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T11:57:25.802295Z",
     "start_time": "2024-11-24T11:57:24.963013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce0fd3009e046e8b8b2f4245922dd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b92f949a82b41af8818f89c444aac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=trained_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=script_args.max_seq_length,\n",
    "    dataset_text_field=\"text\"\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58e2febe62973fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:00:16.333641Z",
     "start_time": "2024-11-24T11:57:27.777724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 26:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.182251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.152922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.132189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.124096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.120005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/.lambda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.23813907360208444, metrics={'train_runtime': 1606.5467, 'train_samples_per_second': 1.083, 'train_steps_per_second': 0.271, 'total_flos': 6.174742865536512e+16, 'train_loss': 0.23813907360208444, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896f2bf7961b20a",
   "metadata": {},
   "source": [
    "### Saving the Model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e085ee32e205e36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:39.895701Z",
     "start_time": "2024-11-24T12:56:32.316660Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(script_args.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4bc74e2d318a6f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:26:32.858733Z",
     "start_time": "2024-11-24T13:26:25.744834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed53c42975e641f0b22031e989e21898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.base_model)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f1be5d08b238865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:29:13.248025Z",
     "start_time": "2024-11-24T13:28:33.425748Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "#base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "model = PeftModel.from_pretrained(base_model_reload, script_args.fine_tuned_model)\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef8f6f9ea883081f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:32:15.459383Z",
     "start_time": "2024-11-24T13:32:02.905762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-2-2b-it-software-model_completion/tokenizer_config.json',\n",
       " 'gemma-2-2b-it-software-model_completion/special_tokens_map.json',\n",
       " 'gemma-2-2b-it-software-model_completion/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(script_args.merged_model)\n",
    "tokenizer.save_pretrained(script_args.merged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b774f4f30109d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2bf916d50be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[\"test\"]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lambda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
